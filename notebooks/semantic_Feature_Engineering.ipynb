{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68ed668-f4e9-46cc-be07-ca935ccd969e",
   "metadata": {},
   "source": [
    "### DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbdb304-9ce6-41b7-9bab-f4674e5b1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import spmatrix\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DistilBertModel\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c23d5-333f-420d-8eff-7a0631207534",
   "metadata": {},
   "source": [
    "#### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6057b8f2-2d00-4c2f-9af7-a06c1d2cc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PATHS\n",
    "DATA_PATH                                                       = '../data/IMDB_Dataset.csv'\n",
    "TEST_DATA_PATH                                                  = '../data/test_data.csv'\n",
    "Emotion_path                                                    = '../data/emotion_lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "\n",
    "\n",
    "# CONFIGURATION VARIABLES\n",
    "BATCH_SIZE                                                      = 250\n",
    "MAX_FEATURES                                                    = 500\n",
    "MODEL_NAME                                                      = \"logistic_regression\"\n",
    "KERNEL_NAME                                                     = None # IF MODEL DOESN'T HAVE KERNEL, MAKE IT NONE\n",
    "GLOVE_MODEL_PATH                                                = \"models/glove.6B.100d.txt\"\n",
    "ELMO_MODEL_URL                                                  = \"https://tfhub.dev/google/elmo/3\"\n",
    "WORD2VEC_MODEL                                                  = \"word2vec-google-news-300\"\n",
    "BERT_CONFIG                                                     = \"../models/BERT/config.json\"\n",
    "BERT_MODEL_SAFETENSORS                                          = \"../models/BERT/model.safetensors\"\n",
    "BERT_TOKENIZER_CONFIG                                           = \"../models/BERT/tokenizer_config.json\"\n",
    "BERT_TOKENIZER                                                  = \"../models/BERT/tokenizer.json\"\n",
    "BERT_VOCABULARY                                                 = \"../models/BERT/vocab.txt\"\n",
    "DISTILBERT_CONFIG                                               = \"models/DISTILBERT/config.json\"\n",
    "DISTILBERT_MODEL_SAFETENSORS                                    = \"models/DISTILBERT/model.safetensors\"\n",
    "DISTILBERT_TOKENIZER_CONFIG                                     = \"models/DISTILBERT/tokenizer_config.json\"\n",
    "DISTILBERT_TOKENIZER                                            = \"models/DISTILBERT/tokenizer.json\"\n",
    "DISTILBERT_VOCABULARY                                           = \"models/DISTILBERT/vocab.txt\"\n",
    "DISTILBERT_PYTORCH_BIN                                          = \"models/DISTILBERT/pytorch_model.bin\"\n",
    "\n",
    "\n",
    "# PARAMETER DICTIONARY\n",
    "MODEL_PARAMS_DICT                                               = {'C'                 : 1.0,\n",
    "                                                                   'tol'               : 0.001,\n",
    "                                                                   'loss'              : 'log_loss',\n",
    "                                                                   'solver'            : 'lbfgs',\n",
    "                                                                   'penalty'           : 'l2',\n",
    "                                                                   'max_iter'          : 1000,\n",
    "                                                                   'max_depth'         : 50,\n",
    "                                                                   'n_neighbors'       : 2,\n",
    "                                                                   'max_features'      : 1,\n",
    "                                                                   'learning_rate'     : 0.01,\n",
    "                                                                   'min_samples_leaf'  : 5,\n",
    "                                                                   'hidden_layer_size' : 1000,\n",
    "                                                                   'l2_regularization' : 0.01,\n",
    "                                                                   'min_samples_split' : 10             \n",
    "                                                                  }\n",
    "\n",
    "\n",
    "# RESULT PATHS\n",
    "SENTIMENT_ANALYSIS_SVM_RBF_RESULT                               = 'results/sentiment_analysis_result_svm_rbf.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_RESULT                              = 'results/sentiment_analysis_result_logistic.csv'\n",
    "SENTIMENT_ANALYSIS_LIGHTGBM_RESULT                              = 'results/sentiment_analysis_result_lightgbm.csv'\n",
    "SENTIMENT_ANALYSIS_ADABOOST_RESULT                              = 'results/sentiment_analysis_result_adaboost.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_SIGMOID_RESULT                           = 'results/sentiment_analysis_result_svm_sigmoid.csv'\n",
    "SENTIMENT_ANALYSIS_RANDOM_FOREST_RESULT                         = 'results/sentiment_analysis_result_random_forest.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_POLYNOMIAL_RESULT                        = 'results/sentiment_analysis_result_svm_polynomial.csv'\n",
    "SENTIMENT_ANALYSIS_GRADIENT_BOOST_RESULT                        = 'results/sentiment_analysis_result_gradient_boost.csv'\n",
    "SENTIMENT_ANALYSIS_LABEL_PROPAGATION_RESULT                     = 'results/sentiment_analysis_result_label_propagation.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_WITH_CUSTOM_FEAT                    = 'results/sentiment_analysis_result_logistic_with_coustom_features.csv'\n",
    "SENTIMENT_ANALYSIS_GAUSSIAN_NAIVE_BAYES_RESULT                  = 'results/sentiment_analysis_result_gaussian_naive_bayes.csv'\n",
    "SENTIMENT_ANALYSIS_MULTILAYER_PERCEPTRON_RESULT                 = 'results/sentiment_analysis_result_multilayer_perceptron.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_RESULT_BY_STAT_FEAT                 = 'results/sentiment_analysis_result_logistic_by_statistical_features.csv'\n",
    "SENTIMENT_ANALYSIS_LIGHTGBM_RESULT_BY_STAT_FEAT                 = 'results/sentiment_analysis_result_lightgbm_by_statistical_features.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_DECISION_TREE_RESULT                = 'results/sentiment_analysis_result_logistic_model_tree.csv'\n",
    "SENTIMENT_ANALYSIS_MULTINOMIAL_NAIVE_BAYES_RESULT               = 'results/sentiment_analysis_result_naive_bayes.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_RBF_RESULT_WITH_CONTEXTUALS              = 'results/sentiment_analysis_result_svm_rbf_with_contextuals.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_RBF_RESULT_WITH_CONTEXTUALS              = 'results/sentiment_analysis_result_svm_rbf_with_contextuals.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_RBF_BY_SEMANTIC_FEAT_RESULT              = 'results/sentiment_analysis_result_svm_rbf_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_ADABOOST_RESULT_WITH_CONTEXTUALS             = 'results/sentiment_analysis_result_adaboost_with_contextuals.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_GAUSSIAN_NAIVE_BAYES_RESULT         = 'results/sentiment_analysis_result_logistic_gaussian_naive_bayes.csv'\n",
    "SENTIMENT_ANALYSIS_HIST_GRADIENT_BOOSTING_CLASSIFIER_RESULT     = 'results/sentiment_analysis_result_hist_gradient_boosting_classifier.csv'\n",
    "SENTIMENT_ANALYSIS_GAUSSIAN_NAIVE_BAYES_RESULT_WITH_CONTEXTUALS = 'results/sentiment_analysis_gaussian_naive_bayes_with_contextuals.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_REG_BY_SEMANTIC_FEAT_RESULT         = 'results/sentiment_analysis_result_logistic_reg_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_RBF_BY_SEMANTIC_FEAT_RESULT              = 'results/sentiment_analysis_result_svm_rbf_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_SVM_SIGMOID_BY_SEMANTIC_FEAT_RESULT          = 'results/sentiment_analysis_result_svm_sigmoid_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_GAUSSIAN_NB_BY_SEMANTIC_FEAT_RESULT          = 'results/sentiment_analysis_result_gaussian_nb_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_LIGHT_GBM_BY_SEMANTIC_FEAT_RESULT            = 'results/sentiment_analysis_result_light_gbm_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_RANDOM_FOREST_BY_SEMANTIC_FEAT_RESULT        = 'results/sentiment_analysis_result_random_forest_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_LABEL_PROP_BY_SEMANTIC_FEAT_RESULT           = 'results/sentiment_analysis_result_label_prop_by_semantic_features.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_REG_BY_ALL_FEAT_RESULT              = 'results/sentiment_analysis_result_logistic_reg_by_all_features.csv'\n",
    "SENTIMENT_ANALYSIS_GAUSSIAN_NB_BY_ALL_FEAT_RESULT               = 'results/sentiment_analysis_result_gaussian_nb_by_all_features.csv'\n",
    "SENTIMENT_ANALYSIS_LABEL_PROP_BY_ALL_FEAT_RESULT                = 'results/sentiment_analysis_result_label_prop_by_all_features.csv'\n",
    "SENTIMENT_ANALYSIS_MULTILAYER_PERCEPTRON_BY_ALL_FEAT_RESULT     = 'results/sentiment_analysis_result_multilayer_perceptron_by_all_features.csv'\n",
    "SENTIMENT_ANALYSIS_LOGISTIC_REG_BY_BERT                         = 'results/sentiment_analysis_result_logistic_regression_by_bert.csv'\n",
    "\n",
    "SAVE_PATH_VARIABLE = SENTIMENT_ANALYSIS_LOGISTIC_REG_BY_BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08fd10-0c75-4c4e-bbf7-0604ac5ceefa",
   "metadata": {},
   "source": [
    "### DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ca5778-935e-44db-9717-1681eb55bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the CSV dataset\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        filepath { str } : \n",
    "\n",
    "    Errors:\n",
    "    -------\n",
    "        DataLoadingError : \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        { DataFrame }    : \n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(filepath_or_buffer = filepath,\n",
    "                                index_col          = None)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    except Exception as DataLoadingError:\n",
    "        raise RuntimeError(f\"Error loading data: {repr(DataLoadingError)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb0bd7-26c2-4645-8b93-e5ea01d7ea10",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828aaa9e-4f6f-43dd-94a6-69c1f8c609c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data through cleaning, tokenization, and normalization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        lemmatizer : WordNetLemmatizer instance for word lemmatization\n",
    "        \n",
    "        stop_words : Set of stopwords to be removed from text\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TextPreprocessor with required NLTK resources\n",
    "        \n",
    "        Raises:\n",
    "        -------\n",
    "            LookupError : If required NLTK resources cannot be downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            \n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        except LookupError as e:\n",
    "            raise\n",
    "    \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text by removing HTML tags, special characters,\n",
    "        and applying text normalization techniques\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            text { str }      : Input text to be cleaned\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError        : If input text is None or empty\n",
    "            \n",
    "            TextCleaningError : If any error occurs at any step of text cleaning process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "                { str }       : Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if ((not text) or (not isinstance(text, str))):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text   = re.sub('<[^>]*>', '', text)\n",
    "            \n",
    "            # Remove special characters and digits\n",
    "            text   = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            text   = text.lower()\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        except Exception as TextCleaningError:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fceb7ed-a7fa-4e59-86bc-092a3f113d21",
   "metadata": {},
   "source": [
    "### SEMANTIC FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb706e3-d7f5-4e1f-9723-53f93499bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- SEMANTIC FEATURE ENGINEERING -----\n",
    "\n",
    "class Semantic_Feature_Engineering:\n",
    "    \n",
    "    \"\"\"\n",
    "    A class for implementing various semantic feature engineering techniques.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents.\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize Semantic_Feature_Engineering with texts and parameters.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents.\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit).\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    # ----- WORD2VEC MODEL -----\n",
    "    \n",
    "    def word2vec_cbow(self, vector_size: int = None, window: int = 5, min_count: int = 1, workers: int = 4) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using Word2Vec (CBOW) and return the vectorizer and feature matrix.\n",
    "    \n",
    "        Arguments:\n",
    "        ----------\n",
    "            vector_size      : Dimensionality of word embeddings (default: 100).\n",
    "            window           : Context window size (default: 5).\n",
    "            min_count        : Ignores words with frequency lower than this (default: 1).\n",
    "            workers          : Number of worker threads to train the model (default: 4).\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - Word2Vec   : The trained Word2Vec model (vectorizer).\n",
    "                - np.ndarray : Document-level feature matrix (each document represented as the average of its word vectors).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Word2Vec (CBOW) features\")\n",
    "            \n",
    "            if vector_size is None:\n",
    "                vector_size = self.max_features\n",
    "            \n",
    "            tokenized_texts         = [doc.split() for doc in self.texts]\n",
    "            \n",
    "            max_features            = self.max_features\n",
    "\n",
    "            w2v_model               = Word2Vec(sentences   = tokenized_texts,\n",
    "                                               vector_size = vector_size,\n",
    "                                               window      = window,\n",
    "                                               min_count   = min_count,\n",
    "                                               workers     = workers,\n",
    "                                               sg          = 0\n",
    "                                               )\n",
    "        \n",
    "            features                = []\n",
    "            \n",
    "            for tokens in tokenized_texts:\n",
    "                vectors             = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "            \n",
    "                if vectors:\n",
    "                    document_vector = np.mean(vectors, axis=0)\n",
    "                else:\n",
    "                    document_vector = np.zeros(vector_size)\n",
    "            \n",
    "                features.append(document_vector)\n",
    "        \n",
    "            feature_matrix          = np.array(features, dtype=np.float32)\n",
    "\n",
    "            if self.max_features is not None and self.max_features < vector_size:\n",
    "                feature_matrix      = feature_matrix[:, :self.max_features]\n",
    "        \n",
    "            print(f\"Created {MAX_FEATURES} Word2Vec (CBOW) features with shape: {feature_matrix.shape}\")\n",
    "        \n",
    "            return w2v_model, feature_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating Word2Vec (CBOW) features: {str(e)}\")\n",
    "    \n",
    "      # ----- GLOVE EMBEDDING -----\n",
    "    \n",
    "    def glove(self, glove_path: str, embedding_dim: int = 100, desired_features: int = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using GloVe and return the feature matrix and embedding dictionary.\n",
    "    \n",
    "        Arguments:\n",
    "        ----------\n",
    "            glove_path        : Path to the GloVe embeddings file.\n",
    "            embedding_dim     : Dimensionality of GloVe embeddings (default: 100).\n",
    "            desired_features  : Number of features to extract (default: 10000).\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - np.ndarray  : Document-level feature matrix (each document represented as the average of its word vectors).\n",
    "                - dict        : The GloVe embedding dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"Creating GloVe features\")\n",
    "            \n",
    "            if desired_features is None:\n",
    "                desired_features = self.max_features\n",
    "        \n",
    "            glove_embeddings               = {}\n",
    "            \n",
    "            with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    values                 = line.split()\n",
    "                    word                   = values[0]\n",
    "                    vector                 = np.asarray(values[1:], dtype='float32')\n",
    "                    glove_embeddings[word] = vector\n",
    "\n",
    "            tokenized_texts                = [doc.split() for doc in self.texts]\n",
    "        \n",
    "            features                       = []\n",
    "        \n",
    "            for tokens in tokenized_texts:\n",
    "                vectors                    = [glove_embeddings[word] for word in tokens if word in glove_embeddings]\n",
    "            \n",
    "                if vectors:\n",
    "                 document_vector           = np.mean(vectors, axis=0)\n",
    "                \n",
    "                else:\n",
    "                   document_vector         = np.zeros(embedding_dim)\n",
    "            \n",
    "            features.append(document_vector)\n",
    "        \n",
    "            feature_matrix                 = np.array(features)\n",
    "        \n",
    "            if desired_features < embedding_dim:\n",
    "                feature_matrix             = feature_matrix[:, :desired_features]\n",
    "        \n",
    "            print(f\"{MAX_FEATURES} GloVe features created with shape: {feature_matrix.shape}\")\n",
    "        \n",
    "            return glove_embeddings, feature_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating GloVe features: {str(e)}\")\n",
    "    \n",
    "    # ----- FAST-TEXT VECTORIZER ------\n",
    "    \n",
    "    def fasttext(self, vector_size: int = None, window: int = 5, min_count: int = 1, workers: int = 4, precision: type = np.float32) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using the FastText model (skip-gram) and return the vectorizer and feature matrix.\n",
    "    \n",
    "        Arguments:\n",
    "        ----------\n",
    "            vector_size       : Dimensionality of word embeddings (default: 10000).\n",
    "            window            : Context window size (default: 5).\n",
    "            min_count         : Ignores words with frequency lower than this (default: 1).\n",
    "            workers           : Number of worker threads to train the model (default: 4).\n",
    "            precision         : Data type for the feature matrix (default: np.float32).\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - FastText    : The trained FastText model (vectorizer).\n",
    "                - np.ndarray  : Document-level feature matrix (each document represented as the average of its word vectors).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating FastText (skip-gram) features\")\n",
    "            \n",
    "            if vector_size is None:\n",
    "                vector_size = self.max_features\n",
    "        \n",
    "            tokenized_texts         = [doc.split() for doc in self.texts]\n",
    "        \n",
    "            max_features            = self.max_features\n",
    "\n",
    "            fasttext_model          = FastText(sentences   = tokenized_texts, \n",
    "                                               vector_size = vector_size, \n",
    "                                               window      = window, \n",
    "                                               min_count   = min_count, \n",
    "                                               workers     = workers, \n",
    "                                               sg          = 1\n",
    "                                               )\n",
    "\n",
    "            features                = []\n",
    "        \n",
    "            for tokens in tokenized_texts:\n",
    "                vectors             = [fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv]\n",
    "            \n",
    "                if vectors:\n",
    "                    document_vector = np.mean(vectors, axis = 0)\n",
    "                else:\n",
    "                    document_vector = np.zeros(vector_size)\n",
    "            \n",
    "                features.append(document_vector)\n",
    "        \n",
    "            feature_matrix           = np.array(features, precision)\n",
    "\n",
    "            if self.max_features is not None and self.max_features < vector_size:\n",
    "                feature_matrix       = feature_matrix[:, :self.max_features]\n",
    "        \n",
    "            print(f\"Created {MAX_FEATURES} FastText (skip-gram) features with shape: {feature_matrix.shape}\")\n",
    "        \n",
    "            return fasttext_model, feature_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating FastText (skip-gram) features: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    # ----- CONTEXTUAL EMBEDDING -----\n",
    "    class Contextual_Embedding:\n",
    "        \"\"\"\n",
    "        \n",
    "        Class to generate Contextual Embeddings using various models using ELMo, BERT, GPT.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, texts: list):\n",
    "            \n",
    "            \"\"\"\n",
    "            Initialize Semantic_Feature_Engineering with texts and parameters.\n",
    "        \n",
    "            Arguments:\n",
    "            ----------\n",
    "                texts        : List of preprocessed text documents.\n",
    "            \n",
    "            Raises:\n",
    "            -------\n",
    "                ValueError   : If texts is empty or parameters are invalid.\n",
    "            \"\"\"\n",
    "            if not texts:\n",
    "                raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "            self.texts        = texts\n",
    "            self.max_features = None\n",
    "        \n",
    "        # ----- ELMO EMBEDDING -----\n",
    "        \n",
    "        def elmo(self, model_url: str, batch_size: int = 32) -> np.ndarray:\n",
    "            \n",
    "            \"\"\"\n",
    "            Generate contextual embeddings using ELMo model and return the feature matrix.\n",
    "            \n",
    "            Arguments:\n",
    "            ----------\n",
    "                options_file   : Path to the ELMo options file.\n",
    "                weight_file    : Path to the ELMo pre-trained weights file.\n",
    "                batch_size     : Batch size for processing text (default: 32).\n",
    "        \n",
    "            Returns:\n",
    "            --------\n",
    "                - np.ndarray   : Document-level feature matrix (average ELMo embeddings for each document).\n",
    "                - elmo         : The trained ELMo model (vectorizer).\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                print(\"Creating ELMo Model Features\")\n",
    "    \n",
    "                elmo_model = hub.load(model_url)\n",
    "\n",
    "            \n",
    "                document_embeddings  = []\n",
    "            \n",
    "                for text in self.texts:\n",
    "                    tokens           = text.split()\n",
    "                \n",
    "                    if len(tokens) == 0:\n",
    "                        document_embeddings.append(np.zeros(1024)) \n",
    "                        continue\n",
    "                \n",
    "                    embeddings       = elmo_model.signatures[\"default\"](input={\"tokens\": tokens})[\"output\"]\n",
    "                \n",
    "                    document_vector  = np.mean(embeddings, axis=0)\n",
    "                \n",
    "                    document_embeddings.append(document_vector)\n",
    "            \n",
    "                feature_matrix       = np.array(document_embeddings, dtype = np.float32)\n",
    "\n",
    "                if self.max_features is not None and self.max_features < 1024:\n",
    "                    feature_matrix   = feature_matrix[:, :self.max_features]\n",
    "                    \n",
    "                print(f\"Created {MAX_FEATURES} ELMo Semantic Features: {feature_matrix.shape}\")\n",
    "            \n",
    "                return feature_matrix, elmo_model\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error in creating ELMo features: {str(e)}\")\n",
    "    \n",
    "    # ------ WORDNET FEATURES -----\n",
    "\n",
    "    def wordnet(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using WordNet, including synonyms, hypernyms, hyponyms, \n",
    "        and meronyms, and return the feature matrix and WordNet corpus.\n",
    "    \n",
    "        Arguments:\n",
    "        ----------\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - list                 : Document-level feature matrix where each document is represented \n",
    "                                         as aggregated WordNet-based features.\n",
    "                - WordNetCorpusReader  : The WordNet corpus used for feature extraction.\n",
    "        \"\"\"\n",
    "    \n",
    "        try:\n",
    "            print(\"Creating WordNet-based semantic features\")\n",
    "        \n",
    "            wordnet_features_list = []\n",
    "\n",
    "            for doc in self.texts:\n",
    "\n",
    "                synonyms           = set()\n",
    "                hypernyms          = set()\n",
    "                hyponyms           = set()\n",
    "                meronyms           = set()\n",
    "\n",
    "                for word in doc.split():\n",
    "                    synsets        = wn.synsets(word)\n",
    "                    \n",
    "                    # SYNONYMS\n",
    "                    for synset in synsets:\n",
    "                        synonyms.update(lemma.name() for lemma in synset.lemmas())\n",
    "                    \n",
    "                    # HYPERNYMS\n",
    "                    for synset in synsets:\n",
    "                        hypernyms.update(lemma.name() for hyper in synset.hypernyms() for lemma in hyper.lemmas())\n",
    "\n",
    "                    # HYPONYMS\n",
    "                    for synset in synsets:\n",
    "                        hyponyms.update(lemma.name() for hypo in synset.hyponyms() for lemma in hypo.lemmas())\n",
    "\n",
    "                    # MERONYMS\n",
    "                    for synset in synsets:\n",
    "                        meronyms.update(lemma.name() for mero in synset.part_meronyms() for lemma in mero.lemmas())\n",
    "\n",
    "                document_features  = {\"synonyms\": list(synonyms),\"hypernyms\": list(hypernyms),\"hyponyms\": list(hyponyms),\"meronyms\": list(meronyms)}\n",
    "\n",
    "                wordnet_features_list.append(document_features)\n",
    "\n",
    "            feature_matrix         = np.array([len(doc_features[\"synonyms\"]) \n",
    "                                               for doc_features in wordnet_features_list], \n",
    "                                              dtype = np.float32).reshape(-1, 1)\n",
    "\n",
    "            print(f\"Created {MAX_FEATURES} WordNet-based features with shape: {feature_matrix.shape}\")\n",
    "\n",
    "            return wn, feature_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating WordNet-based features: {str(e)}\")\n",
    "    \n",
    "    # ----- BERT LEVEL FEATURES -----\n",
    "\n",
    "    def bert(self, max_seq_length: int = 128, max_features: int = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using a pre-trained BERT model and return the transformer, feature matrix, and feature names.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            max_seq_length     : Maximum sequence length for BERT input (default: 128)\n",
    "            max_features       : Number of features to reduce the embeddings and feature names to (default: None, uses MAX_FEATURES).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - BertModel    : The loaded pre-trained BERT model.\n",
    "                - np.ndarray   : Document-level feature matrix (each document represented as the CLS token embedding).\n",
    "                - list         : List of reduced feature names (length equal to max_features).\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            if max_features is None:\n",
    "                max_features = MAX_FEATURES\n",
    "\n",
    "            print(f\"Creating BERT-based features using pre-trained model\")\n",
    "\n",
    "            config                 = PretrainedConfig.from_json_file(BERT_CONFIG)\n",
    "            \n",
    "            tokenizer              = BertTokenizer.from_pretrained(BERT_TOKENIZER_CONFIG,\n",
    "                                                                   tokenizer_file = BERT_TOKENIZER,\n",
    "                                                                   vocab_file     = BERT_VOCABULARY,)\n",
    "            \n",
    "            model                  = BertModel.from_pretrained(BERT_MODEL_SAFETENSORS,\n",
    "                                                               config           = config,\n",
    "                                                               local_files_only = True,)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            tokenized_texts        = [tokenizer\n",
    "                                      (\n",
    "                                          text,\n",
    "                                          max_length     = max_seq_length,\n",
    "                                          padding        = \"max_length\",\n",
    "                                          truncation     = True,\n",
    "                                          return_tensors = \"pt\",\n",
    "                                          )\n",
    "                                      for text in self.texts\n",
    "                                      ]\n",
    "\n",
    "            features               = []\n",
    "            feature_names_set      = set()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for tokenized_text in tokenized_texts:\n",
    "                    input_ids      = tokenized_text[\"input_ids\"]\n",
    "                    attention_mask = tokenized_text[\"attention_mask\"]\n",
    "\n",
    "                    outputs        = model(input_ids      = input_ids,\n",
    "                                           attention_mask = attention_mask)\n",
    "                    \n",
    "                    cls_embedding  = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "                    features.append(cls_embedding.numpy())\n",
    "\n",
    "                tokens             = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "                feature_names_set.update([token for token in tokens if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]])\n",
    "\n",
    "            feature_names          = sorted(feature_names_set)\n",
    "\n",
    "            if len(feature_names) > max_features:\n",
    "                feature_names      = feature_names[:max_features]\n",
    "\n",
    "            feature_matrix         = np.array(features, dtype = np.float32)\n",
    "\n",
    "            print(f\"Reducing features to {max_features} dimensions using SVD...\")\n",
    "            svd                    = TruncatedSVD(n_components = max_features)\n",
    "            reduced_feature_matrix = svd.fit_transform(feature_matrix)\n",
    "\n",
    "            print(f\"Created {max_features} BERT-based features with shape: {reduced_feature_matrix.shape}\")\n",
    "            return model, tokenizer, reduced_feature_matrix, feature_names\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating BERT-based features: {str(e)}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    # ----- DISTILBERT LEVEL FEATURES -----\n",
    "\n",
    "    def distilbert(self, max_seq_length: int = 128, max_features: int = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate semantic features using a pre-trained DistilBERT model and return the transformer, feature matrix, and feature names.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            max_seq_length : Maximum sequence length for DistilBERT input (default: 128).\n",
    "            max_features   : Number of features to reduce the embeddings to (default: None, uses MAX_FEATURES).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - DistilBertModel : The loaded pre-trained DistilBERT model.\n",
    "                - np.ndarray      : Document-level feature matrix (each document represented as the CLS token embedding).\n",
    "                - list            : List of extracted feature names (unique tokens).\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if max_features is None:\n",
    "                max_features = MAX_FEATURES\n",
    "\n",
    "            print(\"Creating DistilBERT-based features using pre-trained model\")\n",
    "\n",
    "            config                  = PretrainedConfig.from_json_file(DISTILBERT_CONFIG)\n",
    "        \n",
    "            tokenizer               = DistilBertTokenizer.from_pretrained(DISTILBERT_TOKENIZER_CONFIG,\n",
    "                                                                          tokenizer_file = DISTILBERT_TOKENIZER,\n",
    "                                                                          vocab_file     = DISTILBERT_VOCABULARY,)\n",
    "        \n",
    "            model                   = DistilBertModel.from_pretrained(DISTILBERT_MODEL_SAFETENSORS,\n",
    "                                                                      config           = config,\n",
    "                                                                      local_files_only = True,)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            tokenized_texts         = [tokenizer(text,\n",
    "                                                 max_length      = max_seq_length,\n",
    "                                                 padding         = \"max_length\",\n",
    "                                                 truncation      = True,\n",
    "                                                 return_tensors  = \"pt\",\n",
    "                                                 )\n",
    "                                       for text in self.texts\n",
    "                                       ]\n",
    "\n",
    "            features                = []\n",
    "            feature_names_set       = set()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for tokenized_text in tokenized_texts:\n",
    "                    input_ids       = tokenized_text[\"input_ids\"]\n",
    "                    attention_mask  = tokenized_text[\"attention_mask\"]\n",
    "\n",
    "                    outputs         = model(input_ids      = input_ids, \n",
    "                                            attention_mask = attention_mask)\n",
    "                    \n",
    "                    cls_embedding   = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "                    features.append(cls_embedding.numpy())\n",
    "\n",
    "                    tokens          = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "                    feature_names_set.update([token for token in tokens if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]])\n",
    "\n",
    "            feature_names           = sorted(feature_names_set)\n",
    "\n",
    "            feature_matrix          = np.array(features, dtype=np.float32)\n",
    "\n",
    "            print(f\"Reducing features to {max_features} dimensions using SVD...\")\n",
    "\n",
    "            svd                     = TruncatedSVD(n_components = max_features)\n",
    "            reduced_feature_matrix  = svd.fit_transform(feature_matrix)\n",
    "\n",
    "            print(f\"Created {max_features} DistilBERT-based features with shape: {reduced_feature_matrix.shape}\")\n",
    "\n",
    "            return model, tokenizer, reduced_feature_matrix, feature_names\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in creating DistilBERT-based features: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96460ed0-8452-48c0-ad20-6a01387a70f2",
   "metadata": {},
   "source": [
    "### TRANFORM VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd523eb-bfe4-4a33-afff-9c28a561ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- DONE BY PRIYAM PAL -----\n",
    "\n",
    "def vector_transform(texts, model, tokenizer = None, model_type = None, max_seq_length = 128):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform a list of texts into sentence embeddings using Word2Vec, FastText, or Transformer-based models.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    \n",
    "        texts           : List of sentences (strings).\n",
    "        model           : Pre-trained Word2Vec/FastText model or Transformer model (e.g., BERT).\n",
    "        tokenizer       : Tokenizer for Transformer models (required if model_type is 'transformer').\n",
    "        model_type      : Type of the model ('word2vec' or 'transformer').\n",
    "        max_seq_length  : Maximum sequence length for Transformer models (default: 128).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    \n",
    "        NumPy array of sentence embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    transformed = []\n",
    "\n",
    "    if model_type == \"word2vec\" or model_type == \"fasttext\":\n",
    "        \n",
    "        for text in texts:\n",
    "            words                = text.split()\n",
    "            word_vectors         = [model.wv[word] for word in words if word in model.wv]\n",
    "\n",
    "            if word_vectors:\n",
    "                sentence_vector  = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                sentence_vector  = np.zeros(model.vector_size)\n",
    "\n",
    "            transformed.append(sentence_vector)\n",
    "\n",
    "    elif model_type == \"distilbert\" or model_type == \"bert\":\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer is required for Transformer models.\")\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                encoded        = tokenizer(text, \n",
    "                                           max_length     = max_seq_length,\n",
    "                                           padding        = \"max_length\",\n",
    "                                           truncation     = True,\n",
    "                                           return_tensors = \"pt\",)\n",
    "                input_ids      = encoded[\"input_ids\"]\n",
    "                attention_mask = encoded[\"attention_mask\"]\n",
    "\n",
    "                outputs        = model(input_ids      = input_ids, \n",
    "                                       attention_mask = attention_mask)\n",
    "\n",
    "                cls_embedding  = outputs.last_hidden_state[:, 0, :].squeeze(0).numpy()\n",
    "                \n",
    "                transformed.append(cls_embedding)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type. Use 'word2vec' or 'transformer'.\")\n",
    "\n",
    "    return np.array(transformed, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2b12f-f3fb-4711-8c1e-ab4709586207",
   "metadata": {},
   "source": [
    "### FEATURE SELECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d06101-5783-4af2-ab54-b3a51771b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureSelector:\n",
    "    \"\"\"\n",
    "    A class for implementing various feature selection techniques for text data\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        X           { spmatrix } : Feature matrix\n",
    "        \n",
    "        y           { ndarray }  : Target labels\n",
    "\n",
    "        feature_names { list }   : Names of features\n",
    "        \n",
    "        n_features    { int }    : Number of features to select\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: spmatrix, y: np.ndarray, feature_names: list, n_features: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureSelector with feature matrix and labels\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            X             : Sparse feature matrix\n",
    "            \n",
    "            y             : Target labels\n",
    "            \n",
    "            feature_names : List of feature names\n",
    "            \n",
    "            n_features    : Number of features to select (default: 100% of input features)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError    : If inputs are invalid or incompatible\n",
    "        \"\"\"\n",
    "        if (X.shape[0] != len(y)):\n",
    "            raise ValueError(\"Number of samples in X and y must match\")\n",
    "            \n",
    "        if (X.shape[1] != len(feature_names)):\n",
    "            raise ValueError(\"Number of features must match length of feature_names\")\n",
    "            \n",
    "        self.X             = X\n",
    "        self.y             = y\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features    = n_features or X.shape[1]  # Default 100% of the input features\n",
    "        \n",
    "        \n",
    "    def chi_square_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform chi-square feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Chi-square scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing chi-square feature selection...\")\n",
    "            \n",
    "            # Scale features to non-negative for chi-square\n",
    "            scaler            = MinMaxScaler()\n",
    "            X_scaled          = scaler.fit_transform(self.X.toarray())\n",
    "            \n",
    "            # Apply chi-square selection\n",
    "            selector          = SelectKBest(score_func = chi2, \n",
    "                                            k          = self.n_features)\n",
    "            \n",
    "            selector.fit(X_scaled, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using chi-square\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def information_gain_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform information gain feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Information gain scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing information gain selection...\")\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            selector          = SelectKBest(score_func = mutual_info_classif, \n",
    "                                            k          = self.n_features)\n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using information gain\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def correlation_based_selection(self, threshold: float = 0.8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform correlation-based feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            threshold { float } : Correlation threshold for feature removal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "               { ndarray }      :  Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing correlation-based selection...\")\n",
    "            \n",
    "            # Convert sparse matrix to dense for correlation calculation\n",
    "            X_dense         = self.X.toarray()\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            corr_matrix     = np.corrcoef(X_dense.T)\n",
    "            \n",
    "            # Find highly correlated feature pairs\n",
    "            high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "            \n",
    "            # Keep track of features to remove\n",
    "            to_remove       = set()\n",
    "            \n",
    "            # For each pair of highly correlated features\n",
    "            for i, j in zip(*high_corr_pairs):\n",
    "                if ((i != j) and (i not in to_remove) and (j not in to_remove)):\n",
    "                    # Calculate correlation with target for both features\n",
    "                    corr_i = mutual_info_score(X_dense[:, i], self.y)\n",
    "                    corr_j = mutual_info_score(X_dense[:, j], self.y)\n",
    "                    \n",
    "                    # Remove feature with lower correlation to target\n",
    "                    if (corr_i < corr_j):\n",
    "                        to_remove.add(i)\n",
    "                        \n",
    "                    else:\n",
    "                        to_remove.add(j)\n",
    "            \n",
    "            # Get selected features\n",
    "            all_features      = set(range(self.X.shape[1]))\n",
    "            selected_features = np.array(list(all_features - to_remove))\n",
    "            \n",
    "            # Select top k features if more than n_features remain\n",
    "            if (len(selected_features) > self.n_features):\n",
    "                # Calculate mutual information for remaining features\n",
    "                mi_scores         = mutual_info_classif(self.X[:, selected_features], self.y)\n",
    "                top_k_idx         = np.argsort(mi_scores)[::-1][:self.n_features]\n",
    "                selected_features = selected_features[top_k_idx]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using correlation-based selection\")\n",
    "            \n",
    "            return selected_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def recursive_feature_elimination(self, estimator = None, cv: int = 5) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform Recursive Feature Elimination with cross-validation\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator  : Classifier to use (default: LogisticRegression)\n",
    "\n",
    "            cv         : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple }  : Tuple containing: - Selected feature indices\n",
    "                                           - Feature importance rankings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing recursive feature elimination...\")\n",
    "            \n",
    "            # Use logistic regression if no estimator provided\n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            # Perform RFE with cross-validation\n",
    "            selector = RFECV(estimator              = estimator,\n",
    "                             min_features_to_select = self.n_features,\n",
    "                             cv                     = cv,\n",
    "                             n_jobs                 = -1)\n",
    "            \n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and rankings\n",
    "            selected_features = np.where(selector.support_)[0]\n",
    "            rankings          = selector.ranking_\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using RFE\")\n",
    "            \n",
    "            return selected_features, rankings\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "           \n",
    "        \n",
    "    def forward_selection(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing forward selection...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            selected_features  = list()\n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            for i in tqdm(range(self.n_features)):\n",
    "                best_score   = -np.inf\n",
    "                best_feature = None\n",
    "                \n",
    "                # Try adding each remaining feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = selected_features + [feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score   = avg_score\n",
    "                        best_feature = feature\n",
    "                \n",
    "                if (best_feature is not None):\n",
    "                    selected_features.append(best_feature)\n",
    "                    remaining_features.remove(best_feature)\n",
    "                \n",
    "            print(f\"Selected {len(selected_features)} features using forward selection\")\n",
    "            \n",
    "            return np.array(selected_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def backward_elimination(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward feature elimination\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing backward elimination...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            while len(remaining_features) > self.n_features:\n",
    "                best_score    = -np.inf\n",
    "                worst_feature = None\n",
    "                \n",
    "                # Try removing each feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = [f for f in remaining_features if f != feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores           = cross_val_score(estimator, \n",
    "                                                       X_subset, \n",
    "                                                       self.y,\n",
    "                                                       cv      = cv, \n",
    "                                                       scoring = 'accuracy')\n",
    "                \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score    = avg_score\n",
    "                        worst_feature = feature\n",
    "                \n",
    "                if (worst_feature is not None):\n",
    "                    remaining_features.remove(worst_feature)\n",
    "            \n",
    "            print(f\"Selected {len(remaining_features)} features using backward elimination\")\n",
    "            return np.array(remaining_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87782e7-b8fb-4f67-bad3-0935c9c43e91",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "847a9057-92ca-46d3-a027-36a6ab858080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for training and evaluating sentiment analysis models, including testing on unseen data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, feature_eng, selected_feature_indices, test_size=0.2, random_state=42, vectorizers=None):\n",
    "        \"\"\"\n",
    "        Initialize the SentimentAnalyzer by splitting the data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            X                        : Feature matrix (sparse matrix or ndarray)\n",
    "            \n",
    "            y                        : Target labels (array-like)\n",
    "            \n",
    "            feature_eng              : Instance of TextFeatureEngineering\n",
    "            \n",
    "            vectorizers              : Tuple of vectorizers used for feature transformation\n",
    "            \n",
    "            selected_feature_indices : Indices of selected features after feature selection\n",
    "            \n",
    "            test_size                : Proportion of data to use for testing (default: 0.2)\n",
    "            \n",
    "            random_state             : Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size    = test_size, \n",
    "                                                                                random_state = random_state)\n",
    "        \n",
    "        self.feature_eng                                     = feature_eng\n",
    "        self.vectorizers                                     = vectorizers\n",
    "        self.selected_feature_indices                        = selected_feature_indices\n",
    "\n",
    "        \n",
    "    def train_model(self, model_type:str = \"logistic_regression\", kernel:str = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train a sentiment analysis model\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model_type { str } : Type of model to train (e.g: \"logistic_regression\", \"svm\", \"random_forest\")\n",
    "            \n",
    "            kernel     { str } : Kernel type for SVM (e.g., \"linear\", \"poly\", \"rbf\", \"sigmoid\")\n",
    "            \n",
    "            kwargs             : Additional arguments for the model initialization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        if (model_type == \"logistic_regression\"):\n",
    "            model = LogisticRegression(max_iter = MODEL_PARAMS_DICT['max_iter'], \n",
    "                                       **kwargs)\n",
    "            \n",
    "        elif (model_type == \"svm\"):\n",
    "            \n",
    "            if (kernel is None):\n",
    "                # Default kernel\n",
    "                kernel = \"rbf\"  \n",
    "                \n",
    "            model = SVC(kernel = kernel, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"random_forest\"):\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "\n",
    "        elif (model_type == \"gaussian_naive_bayes\"):\n",
    "            if issparse(self.X_train):\n",
    "                self.X_train = self.X_train.toarray()\n",
    "\n",
    "            model = GaussianNB(**kwargs)\n",
    "            \n",
    "        elif (model_type == \"multinomial_naive_bayes\"):\n",
    "            model = MultinomialNB(**kwargs)\n",
    "\n",
    "        elif (model_type == \"adaboost\"):\n",
    "            model = AdaBoostClassifier(**kwargs)\n",
    "\n",
    "        elif (model_type == \"gradient_boost\"):\n",
    "            model = GradientBoostingClassifier(**kwargs)\n",
    "\n",
    "        elif (model_type == \"lightgbm\"):\n",
    "            model = LGBMClassifier(**kwargs)\n",
    "\n",
    "        elif (model_type == 'label_propagation'):\n",
    "            model = LabelPropagation(kernel      = 'knn',\n",
    "                                     n_neighbors = MODEL_PARAMS_DICT['n_neighbors'], \n",
    "                                     max_iter    = MODEL_PARAMS_DICT['max_iter'], \n",
    "                                     tol         = MODEL_PARAMS_DICT['tol'],\n",
    "                                     **kwargs\n",
    "                                    )\n",
    "\n",
    "        elif (model_type == \"multilayer_perceptron\"):\n",
    "            model = MLPClassifier(hidden_layer_sizes = MODEL_PARAMS_DICT['hidden_layer_size'], \n",
    "                                  max_iter           = MODEL_PARAMS_DICT['max_iter'], \n",
    "                                  **kwargs)\n",
    "\n",
    "        elif (model_type == 'hist_gradient_boosting_classifier'):\n",
    "            if issparse(self.X_train):\n",
    "                self.X_train = self.X_train.toarray()\n",
    "\n",
    "            model = HistGradientBoostingClassifier(loss              = MODEL_PARAMS_DICT['loss'], \n",
    "                                                   learning_rate     = MODEL_PARAMS_DICT['learning_rate'], \n",
    "                                                   max_iter          = MODEL_PARAMS_DICT['max_iter'],\n",
    "                                                   min_samples_leaf  = MODEL_PARAMS_DICT['min_samples_leaf'],\n",
    "                                                   l2_regularization = MODEL_PARAMS_DICT['l2_regularization'],\n",
    "                                                   max_features      = MODEL_PARAMS_DICT['max_features'],\n",
    "                                                   **kwargs)\n",
    "\n",
    "\n",
    "        elif (model_type == \"logistic_decision_tree\"):\n",
    "            # Create a logistic regression model\n",
    "            logistic_model      = LogisticRegression(max_iter = MODEL_PARAMS_DICT['max_iter'], \n",
    "                                                     penalty  = MODEL_PARAMS_DICT['penalty'], \n",
    "                                                     C        = MODEL_PARAMS_DICT['C'], \n",
    "                                                     solver   = MODEL_PARAMS_DICT['solver'],\n",
    "                                                     **kwargs)\n",
    "\n",
    "            # Create a decision tree model\n",
    "            decision_tree_model = DecisionTreeClassifier(max_depth         = MODEL_PARAMS_DICT['max_depth'], \n",
    "                                                         min_samples_split = MODEL_PARAMS_DICT['min_samples_split'], \n",
    "                                                         min_samples_leaf  = MODEL_PARAMS_DICT['min_samples_leaf'],\n",
    "                                                         **kwargs)\n",
    "\n",
    "            # Combine them in a stacking model\n",
    "            model               = StackingClassifier(estimators      = [('decision_tree', decision_tree_model)], \n",
    "                                                     final_estimator = logistic_model, \n",
    "                                                     stack_method    = 'predict_proba',\n",
    "                                                     **kwargs)\n",
    "        \n",
    "        elif (model_type == \"logistic_gaussian_naive_bayes\"):\n",
    "            # Create a logistic regression model\n",
    "            logistic_model = LogisticRegression(max_iter = MODEL_PARAMS_DICT['max_iter'], \n",
    "                                                penalty  = MODEL_PARAMS_DICT['penalty'], \n",
    "                                                C        = MODEL_PARAMS_DICT['C'], \n",
    "                                                solver   = MODEL_PARAMS_DICT['solver'],\n",
    "                                                **kwargs)\n",
    "\n",
    "            # Gaussian Naive Bayes does not work with sparse matrices, so convert to dense if needed\n",
    "            if issparse(self.X_train):\n",
    "                self.X_train = self.X_train.toarray()\n",
    "\n",
    "            # Create Gaussian Naive Bayes model\n",
    "            gaussian_naive_bayes = GaussianNB()\n",
    "\n",
    "            # Combine them in a stacking model (Logistic Regression as base model, Gaussian Naive Bayes as final estimator)\n",
    "            model                = StackingClassifier(estimators      = [('logistic_regression', logistic_model)], \n",
    "                                                      final_estimator = gaussian_naive_bayes, \n",
    "                                                      stack_method    = 'predict_proba',\n",
    "                                                      **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose from : 'logistic_regression', 'svm', 'random_forest', 'multinomial_naive_bayes', \\\n",
    "                             'gaussian_naive_bayes', 'adaboost', 'gradient_boost', 'lightgbm', 'logistic_decision_tree', 'logistic_gaussian_naive_bayes', 'multilayer_perceptron\".replace('  ', ''))\n",
    "\n",
    "        print(f\"Training {model_type}...\")\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model on the test set\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model : Trained model\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "\n",
    "        if (isinstance(model, StackingClassifier)):\n",
    "            if (isinstance(model.final_estimator_, GaussianNB)):\n",
    "                # Handle dense conversion for GaussianNB final estimator in stacking model\n",
    "                X_test_dense = self.X_test.toarray() if hasattr(self.X_test, \"toarray\") else self.X_test\n",
    "                y_pred        = model.predict(X_test_dense)\n",
    "        \n",
    "        elif ((isinstance(model, GaussianNB)) or (isinstance(model, HistGradientBoostingClassifier))):\n",
    "            # Directly handle GaussianNB or HistGradientBoostingClassifier\n",
    "            X_test_dense = self.X_test.toarray() if hasattr(self.X_test, \"toarray\") else self.X_test\n",
    "            y_pred       = model.predict(X_test_dense)\n",
    "        \n",
    "        else:\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report   = classification_report(self.y_test, y_pred)\n",
    "        cm       = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\"accuracy\"              : accuracy,\n",
    "                \"classification_report\" : report,\n",
    "                \"confusion_matrix\"      : cm,\n",
    "               }\n",
    "\n",
    "    \n",
    "    def test_on_unseen_data(self, model, unseen_texts, unseen_labels=None, **preprocessed_features):\n",
    "        \"\"\"\n",
    "        Test the model on unseen data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model                 : Trained model\n",
    "            \n",
    "            unseen_texts          : List of unseen text data\n",
    "\n",
    "            unseen_labels         : True labels for the unseen data\n",
    "\n",
    "            preprocessed_features : Preprocessed feature matrices (e.g., binary_features, tfidf_features, bm25_features, etc.)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Predictions for the unseen data\n",
    "        \"\"\"\n",
    "        print(\"Processing unseen data...\")\n",
    "\n",
    "        # Dynamically combine all passed feature matrices\n",
    "        unseen_combined_features = hstack([preprocessed_features[key] for key in preprocessed_features])\n",
    "\n",
    "        # Select features using the indices chosen during feature selection\n",
    "        unseen_selected_features = unseen_combined_features[:, self.selected_feature_indices]\n",
    "\n",
    "        # Convert unseen features to dense for Gaussian Naive Bayes\n",
    "        if ((isinstance(model, GaussianNB)) or (isinstance(model, HistGradientBoostingClassifier))):\n",
    "            unseen_selected_features = unseen_selected_features.toarray() if hasattr(unseen_selected_features, \"toarray\") else unseen_selected_features\n",
    "        \n",
    "        elif (isinstance(model, StackingClassifier)):\n",
    "            if (isinstance(model.final_estimator_, GaussianNB)):\n",
    "                unseen_selected_features = unseen_selected_features.toarray() if hasattr(unseen_selected_features, \"toarray\") else unseen_selected_features\n",
    "\n",
    "        # Predict sentiments\n",
    "        predictions              = model.predict(unseen_selected_features)\n",
    "\n",
    "        # Print predictions\n",
    "        print(\"Predictions on Unseen Data:\")\n",
    "        for text, pred in zip(unseen_texts, predictions):\n",
    "            print(f\"Text: {text}\\nPredicted Sentiment: {pred}\\n\")\n",
    "\n",
    "        # Compute accuracy if unseen_labels are provided\n",
    "        if unseen_labels is not None:\n",
    "            print(f\"Number of unseen_labels: {len(unseen_labels)}\")\n",
    "\n",
    "            if (len(unseen_labels) != len(predictions)):\n",
    "                raise ValueError(\"The number of unseen_labels must match the number of predictions.\")\n",
    "                \n",
    "            accuracy = accuracy_score(unseen_labels, predictions)\n",
    "            print(f\"Accuracy on Unseen Data : {accuracy:.4f}\")\n",
    "            return predictions, accuracy\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b706af-a3c9-4be1-8a43-1dec89a3202e",
   "metadata": {},
   "source": [
    "### MAIN CONTROLLER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c632707-d911-407d-9452-f06cd1637136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "imdb_ratings_data                            = load_csv_data(filepath = DATA_PATH)\n",
    "\n",
    "# PREPROCESSING THE DATA\n",
    "preprocessor                                 = TextPreprocessor()\n",
    "imdb_ratings_data[\"clean_text\"]              = imdb_ratings_data[\"review\"].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a48d18d-2a86-4898-9333-d715c074c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISING THE SEMANTIC FEATURE ENGINEERING CLASS\n",
    "semantic_Feature_Eng                    = Semantic_Feature_Engineering(texts        = imdb_ratings_data['clean_text'].tolist(), \n",
    "                                                                       max_features = MAX_FEATURES\n",
    "                                                                       )\n",
    "\n",
    "# INITIALISING THE CONTEXTUAL EMBEDDING CLASS INSIDE SEMANTIC FEATURE ENGINEERING CLASS\n",
    "contextual_Embedding                    = semantic_Feature_Eng.Contextual_Embedding(texts = imdb_ratings_data['clean_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9ecc18-b917-4b71-b718-83fa80c2547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BERT-based features using pre-trained model\n",
      "Original number of tokens: 65\n",
      "Reducing features to 500 dimensions using SVD...\n",
      "Created 500 BERT-based features with shape: (50000, 500)\n",
      "Combined Feature Matrix Shape: (50000, 500)\n"
     ]
    }
   ],
   "source": [
    "# ----------  CREATING THE FEATURES ----------\n",
    "\n",
    "\n",
    "# ----- WORD - LEVEL FEATURES -----\n",
    "\n",
    "# # count_vectorizer, count_features             = word_level_feature_eng.create_count_bow()\n",
    "# freq_vectorizer, freq_features               = word_level_feature_eng.create_frequency_bow()\n",
    "# # binary_vectorizer, binary_features           = word_level_feature_eng.create_binary_bow()\n",
    "# # tfidf_vectorizer, tfidf_features             = word_level_feature_eng.create_tfidf()\n",
    "# std_tfidf_vectorizer, std_tfidf_features     = word_level_feature_eng.create_standardized_tfidf()\n",
    "# # bm25_transformer, bm25_features              = word_level_feature_eng.create_bm25()\n",
    "# bm25f_transformer, bm25f_features            = word_level_feature_eng.create_bm25f()\n",
    "# # bm25l_transformer, bm25l_features            = word_level_feature_eng.create_bm25l()\n",
    "# # bm25t_transformer, bm25t_features            = word_level_feature_eng.create_bm25t()\n",
    "# bm25_plus_transformer, bm25_plus_features    = word_level_feature_eng.create_bm25_plus()\n",
    "# skipgrams_vectorizer, skipgram_features      = word_level_feature_eng.create_skipgrams()\n",
    "# pos_ngram_vectorizer, pos_ngram_features     = word_level_feature_eng.create_positional_ngrams()\n",
    "\n",
    "# # ----- CONTEXTUALS FEATURES -----\n",
    "\n",
    "# window_vectorizer, window_features           = contextuals.window_based()\n",
    "# # position_vectorizer, positional_features     = contextuals.position_based()\n",
    "# ngram_vectorizer, trigrams                   = contextuals.generate_ngrams()\n",
    "# # cross_doc_vectorizer, tfidf_matrix           = contextuals.cross_document()\n",
    "\n",
    "\n",
    "# ----- SEMANTIC FEATURES -----\n",
    "\n",
    "# w2v_model, w2v_features                         = semantic_Feature_Eng.word2vec_cbow()\n",
    "# glove_embeddings, glove_model                   = semantic_Feature_Eng.glove(GLOVE_MODEL_PATH)\n",
    "# fasttext_model, fasttext_features               = semantic_Feature_Eng.fasttext()\n",
    "# wordnet_model, wordnet_features                 = semantic_Feature_Eng.wordnet()\n",
    "# bert_model, bert_features, bert_feature_names   = semantic_Feature_Eng.bert()\n",
    "\n",
    "\n",
    "# CONVERTING THE FEATURES INTO FEATURE MATRIX\n",
    "# w2v_sparse                              = csr_matrix(w2v_features)\n",
    "# glove_sparse                            = csr_matrix(glove_embeddings)\n",
    "# fasttext_sparse                         = csr_matrix(fasttext_features)\n",
    "# bert_sparse                             = csr_matrix(bert_features)\n",
    "\n",
    "# # COMBINING THE SEMANTIC, WORD - LEVEL FEATURES, CONTEXTUAL FEATURES\n",
    "# combined_features                       = hstack([w2v_sparse, \n",
    "#                                                   # glove_sparse, \n",
    "#                                                   # fasttext_sparse,\n",
    "#                                                   freq_features, \n",
    "#                                                   std_tfidf_features,\n",
    "#                                                   bm25f_features,\n",
    "#                                                   bm25_plus_features,\n",
    "#                                                   skipgram_features,\n",
    "#                                                   pos_ngram_features,\n",
    "#                                                   window_features,\n",
    "#                                                   # positional_features,\n",
    "#                                                   trigrams,\n",
    "#                                                   # tfidf_matrix \n",
    "#                                                   ])\n",
    "\n",
    "bert_model, bert_tokenizer, bert_features, bert_feature_names = semantic_Feature_Eng.bert()\n",
    "bert_sparse                                   = csr_matrix(bert_features)\n",
    "\n",
    "combined_features                             = hstack([bert_sparse])\n",
    "\n",
    "print(f\"Combined Feature Matrix Shape: {combined_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc954e8c-1760-4e88-8cb5-9a18aea47c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 65 BERT feature names:\n",
      "Number of feature names extracted: 65\n"
     ]
    }
   ],
   "source": [
    "# ----- EXTRACTING THE FEATURE NAMES -----\n",
    "\n",
    "feature_names                            = []\n",
    "\n",
    "# w2v_feature_names                        = w2v_model.wv.index_to_key[:MAX_FEATURES]\n",
    "# fasttext_feature_names                   = fasttext_model.wv.index_to_key[:MAX_FEATURES]\n",
    "\n",
    "# # COMBINING THE FEATURE NAMES OF SEMANTIC, WORD-LEVEL, CONTEXTUAL FEATURES\n",
    "# feature_names                            = (list(w2v_feature_names) + \n",
    "#                                             # list(fasttext_feature_names) + \n",
    "#                                             list(freq_vectorizer.get_feature_names_out()) +\n",
    "#                                             list(std_tfidf_vectorizer.get_feature_names_out()) +\n",
    "#                                             list(bm25f_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "#                                             list(bm25_plus_transformer.count_vectorizer.get_feature_names_out()) +\n",
    "#                                             list(skipgrams_vectorizer.get_feature_names_out()) +\n",
    "#                                             list(pos_ngram_vectorizer.get_feature_names_out()) + \n",
    "#                                             list(window_vectorizer.get_feature_names_out()) +\n",
    "#                                             # list(position_vectorizer.get_feature_names_out()) +\n",
    "#                                             # list(cross_doc_vectorizer.get_feature_names_out())\n",
    "#                                             list(ngram_vectorizer.get_feature_names_out())\n",
    "#                                            )\n",
    "\n",
    "feature_names = list(bert_feature_names)\n",
    "print(f\"Extracted {len(bert_feature_names)} BERT feature names:\")\n",
    "\n",
    "print(f\"Number of feature names extracted: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15973be9-5f24-4cd6-9913-590a40233a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features must match length of feature_names",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ----- SELECTING THE FEATURES -----\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# FEATURE SELECTOR\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m feature_selector                         \u001b[38;5;241m=\u001b[39m TextFeatureSelector(X             \u001b[38;5;241m=\u001b[39m combined_features,\n\u001b[1;32m      5\u001b[0m                                                                y             \u001b[38;5;241m=\u001b[39m imdb_ratings_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      6\u001b[0m                                                                feature_names \u001b[38;5;241m=\u001b[39m feature_names,\n\u001b[1;32m      7\u001b[0m                                                                n_features    \u001b[38;5;241m=\u001b[39m MAX_FEATURES\n\u001b[1;32m      8\u001b[0m                                                                )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# CHI-SQUARE SELECTION\u001b[39;00m\n\u001b[1;32m     11\u001b[0m chi_square_features, chi_square_scores   \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mchi_square_selection()\n",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m, in \u001b[0;36mTextFeatureSelector.__init__\u001b[0;34m(self, X, y, feature_names, n_features)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples in X and y must match\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(feature_names)):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features must match length of feature_names\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX             \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my             \u001b[38;5;241m=\u001b[39m y\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features must match length of feature_names"
     ]
    }
   ],
   "source": [
    "# ----- SELECTING THE FEATURES -----\n",
    "\n",
    "# FEATURE SELECTOR\n",
    "feature_selector                         = TextFeatureSelector(X             = combined_features,\n",
    "                                                               y             = imdb_ratings_data['sentiment'].values,\n",
    "                                                               feature_names = feature_names,\n",
    "                                                               n_features    = MAX_FEATURES\n",
    "                                                               )\n",
    "\n",
    "# CHI-SQUARE SELECTION\n",
    "chi_square_features, chi_square_scores   = feature_selector.chi_square_selection()\n",
    "\n",
    "# COMBINING THE FEATURES\n",
    "selected_combined_features               = combined_features[:, chi_square_features]\n",
    "\n",
    "\n",
    "# # VECTORIZERS TUPLE\n",
    "# vectorizers_tuple                        = (w2v_model,\n",
    "#                                             # fasttext_model,\n",
    "#                                             freq_vectorizer,\n",
    "#                                             std_tfidf_vectorizer, \n",
    "#                                             bm25f_transformer,\n",
    "#                                             bm25_plus_features, \n",
    "#                                             skipgrams_vectorizer, \n",
    "#                                             pos_ngram_vectorizer,\n",
    "#                                             window_vectorizer,\n",
    "#                                             # position_vectorizer,\n",
    "#                                             ngram_vectorizer,\n",
    "#                                             # cross_doc_vectorizer\n",
    "#                                             )\n",
    "\n",
    "vectorizers_tuple = (bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b266e0-bb73-4248-9c93-fc4ce393b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- SENTIMENTAL ANALYSIS -----\n",
    "\n",
    "sentiment_analyzer                       = SentimentAnalyzer(X                        = selected_combined_features, \n",
    "                                                             y                        = imdb_ratings_data[\"sentiment\"].values,\n",
    "                                                             feature_eng              = semantic_Feature_Eng,\n",
    "                                                             vectorizers              = vectorizers_tuple,\n",
    "                                                             selected_feature_indices = chi_square_features\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a13503-6136-4665-bc44-69a1a84e2af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- MODEL FITTING ON TRAINING DATA -----\n",
    "\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "trained_model                             = sentiment_analyzer.train_model(model_type = MODEL_NAME, kernel = KERNEL_NAME)\n",
    "\n",
    "# EVALUATING THE RESULTS OF THE MODEL\n",
    "evaluation_results                        = sentiment_analyzer.evaluate_model(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47839cb7-a012-4259-9497-48e1a6ce7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data                                 = load_csv_data(filepath = TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523313c7-f2de-4e84-88b6-815d0e059951",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features = vector_transform(list(test_data['Text'], bert_model, bert_tokenizer, model_type = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3cbd5-4bca-4a6d-b559-c38d766bae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINING THE FEATURES\n",
    "combined_features_transformed             = np.hstack([w2v_features_transformed])\n",
    "\n",
    "# CONVERTING TO SPARSE MATRIX\n",
    "combined_features_sparse                  = csr_matrix(combined_features_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7211b-3656-4094-a3b5-d6dcf2630215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- PREDICT THE TRAINED MODEL USING UNSEEN DATA USING SEMANTIC, WORD-LEVEL, CONTEXTUAL FEATURES -----\n",
    "\n",
    "model_predictions, unseen_accuracy        = sentiment_analyzer.test_on_unseen_data(model               = trained_model, \n",
    "                                                                                   unseen_texts        = list(test_data['Text']),\n",
    "                                                                                   unseen_labels       = list(test_data['Sentiment']),\n",
    "                                                                                   combined_features   = combined_features_sparse,\n",
    "                                                                                   )\n",
    "\n",
    "\n",
    "all_test_data                              = {'texts'            : list(test_data['Text']), \n",
    "                                              'true_labels'      : list(test_data['Sentiment']), \n",
    "                                              'predicted_labels' : list(model_predictions)\n",
    "                                              }\n",
    "\n",
    "model_prediction_df                        = pd.DataFrame.from_dict(data   = all_test_data, \n",
    "                                                                    orient = 'index').T\n",
    "\n",
    "model_prediction_df.to_csv(path_or_buf     = SAVE_PATH_VARIABLE,\n",
    "                           index           = False)\n",
    "\n",
    "print (f\"Sentiment Analysis result by {MODEL_NAME} Model of Max Features {MAX_FEATURES} has been saved to : {SAVE_PATH_VARIABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c65c4-75c8-423d-b2cc-ad15aac08b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
