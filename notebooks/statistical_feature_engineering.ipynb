{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1bed2139-213a-44eb-9abc-d77c34228311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import textstat\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import tqdm\n",
    "from scipy.sparse import spmatrix\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c290cf-1ed0-4edf-9b75-9e04f5d5ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Project/NLP_Tasks/data/IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcd5d1f-03b2-457a-92f5-d95b0a962fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a5f84d-d8a1-4d60-99f7-374f39792cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/70/zpz26h854cq9xfbs4cp9v8hh0000gn/T/ipykernel_1471/220301379.py:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text   = re.sub('[^a-zA-Z\\s]', '', text)\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data through cleaning, tokenization, and normalization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        lemmatizer : WordNetLemmatizer instance for word lemmatization\n",
    "        \n",
    "        stop_words : Set of stopwords to be removed from text\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TextPreprocessor with required NLTK resources\n",
    "        \n",
    "        Raises:\n",
    "        -------\n",
    "            LookupError : If required NLTK resources cannot be downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            \n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        except LookupError as e:\n",
    "            raise\n",
    "    \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text by removing HTML tags, special characters,\n",
    "        and applying text normalization techniques\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            text { str }      : Input text to be cleaned\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError        : If input text is None or empty\n",
    "            \n",
    "            TextCleaningError : If any error occurs at any step of text cleaning process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "                { str }       : Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if ((not text) or (not isinstance(text, str))):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text   = re.sub('<[^>]*>', '', text)\n",
    "            \n",
    "            # Remove special characters and digits\n",
    "            text   = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            text   = text.lower()\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        except Exception as TextCleaningError:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3079e07f-8006-4098-989b-ceeae2a34849",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "data['cleaned_review'] = data['review'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92eb3e12-ed34-4795-a027-675fd14c2bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  one reviewer mentioned watching oz episode you...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cba538-3627-4bbd-9251-80f3a66b7338",
   "metadata": {},
   "source": [
    "# Statistical Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe12c65-b093-48e9-b2c5-abd62f384b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_statistics(df):\n",
    "    df['char_count'] = df['cleaned_review'].apply(lambda x: len(x))\n",
    "    df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "    df['sent_count'] = df['review'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    df['AWL'] = df['char_count'].div(df['word_count'])\n",
    "    df['ASL'] = df['word_count'].div(df['sent_count'])\n",
    "    df['unique_word_count'] = df['cleaned_review'].apply(lambda x: len(set(word_tokenize(x))))\n",
    "    df['UWR'] = df['unique_word_count'].div(df['word_count'])\n",
    "    df['FRE'] = (206.835 - 1.015*df['ASL'] - 84.6*df['AWL'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7ce9ed-8387-407b-a2a6-50198afea37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>AWL</th>\n",
       "      <th>ASL</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>UWR</th>\n",
       "      <th>FRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "      <td>1125</td>\n",
       "      <td>167</td>\n",
       "      <td>10</td>\n",
       "      <td>6.736527</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>140</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>-380.025680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>640</td>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>7.619048</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>-449.916429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>580</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>6.823529</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>81</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>-392.004338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>446</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>6.757576</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>-376.020909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>851</td>\n",
       "      <td>125</td>\n",
       "      <td>9</td>\n",
       "      <td>6.808000</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>101</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>-383.219022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought movie right good job wasnt creative or...</td>\n",
       "      <td>540</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>6.428571</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>72</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-347.679643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>392</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>7.127273</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>50</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>-414.740606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>catholic taught parochial elementary school nu...</td>\n",
       "      <td>805</td>\n",
       "      <td>115</td>\n",
       "      <td>6</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>96</td>\n",
       "      <td>0.834783</td>\n",
       "      <td>-404.819167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im going disagree previous comment side maltin...</td>\n",
       "      <td>819</td>\n",
       "      <td>114</td>\n",
       "      <td>8</td>\n",
       "      <td>7.184211</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>-415.412961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one expects star trek movie high art fan expec...</td>\n",
       "      <td>424</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>6.235294</td>\n",
       "      <td>22.666667</td>\n",
       "      <td>54</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>-343.677549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                          cleaned_review  char_count  \\\n",
       "0      one reviewer mentioned watching oz episode you...        1125   \n",
       "1      wonderful little production filming technique ...         640   \n",
       "2      thought wonderful way spend time hot summer we...         580   \n",
       "3      basically there family little boy jake think t...         446   \n",
       "4      petter matteis love time money visually stunni...         851   \n",
       "...                                                  ...         ...   \n",
       "49995  thought movie right good job wasnt creative or...         540   \n",
       "49996  bad plot bad dialogue bad acting idiotic direc...         392   \n",
       "49997  catholic taught parochial elementary school nu...         805   \n",
       "49998  im going disagree previous comment side maltin...         819   \n",
       "49999  one expects star trek movie high art fan expec...         424   \n",
       "\n",
       "       word_count  sent_count       AWL        ASL  unique_word_count  \\\n",
       "0             167          10  6.736527  16.700000                140   \n",
       "1              84           7  7.619048  12.000000                 76   \n",
       "2              85           4  6.823529  21.250000                 81   \n",
       "3              66           6  6.757576  11.000000                 53   \n",
       "4             125           9  6.808000  13.888889                101   \n",
       "...           ...         ...       ...        ...                ...   \n",
       "49995          84           8  6.428571  10.500000                 72   \n",
       "49996          55           3  7.127273  18.333333                 50   \n",
       "49997         115           6  7.000000  19.166667                 96   \n",
       "49998         114           8  7.184211  14.250000                109   \n",
       "49999          68           3  6.235294  22.666667                 54   \n",
       "\n",
       "            UWR         FRE  \n",
       "0      0.838323 -380.025680  \n",
       "1      0.904762 -449.916429  \n",
       "2      0.952941 -392.004338  \n",
       "3      0.803030 -376.020909  \n",
       "4      0.808000 -383.219022  \n",
       "...         ...         ...  \n",
       "49995  0.857143 -347.679643  \n",
       "49996  0.909091 -414.740606  \n",
       "49997  0.834783 -404.819167  \n",
       "49998  0.956140 -415.412961  \n",
       "49999  0.794118 -343.677549  \n",
       "\n",
       "[50000 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c86899b-b58d-42ca-bb4f-b555dc36ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statistical_Feature_Engineering():\n",
    "    \"\"\"\n",
    "    A class for statistical feature engineering.\n",
    "    Attributes:\n",
    "    ----------\n",
    "    vectorizer : CountVectorizer instance for text vectorization.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,max_features=1000):\n",
    "        \"\"\"\n",
    "        Intializes the Statistical_Feature_Engineering.\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = CountVectorizer(max_features=self.max_features)\n",
    "\n",
    "    def document_statistics(self,df):\n",
    "        \"\"\"\n",
    "        Calculates basic documnet statistics i.e. Character Count, Word Count, Sentence Count, Average Word Length(AWL), \n",
    "        Average Sentence Length(ASL), Unique Word Ratio(UWR).\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        df {DataFrame} : Output data with the calculated document statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df['char_count'] = df['cleaned_review'].apply(lambda x: len(x))\n",
    "        df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "        df['sent_count'] = df['review'].apply(lambda x: len(sent_tokenize(x)))\n",
    "        df['AWL'] = df['char_count'].div(df['word_count'])\n",
    "        df['ASL'] = df['word_count'].div(df['sent_count'])\n",
    "        df['unique_word_count'] = df['cleaned_review'].apply(lambda x: len(set(word_tokenize(x))))\n",
    "        df['UWR'] = df['unique_word_count'].div(df['word_count'])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def readability_score(self,df, score='FRE'):\n",
    "        \"\"\"\n",
    "        Calculates the readability scores i.e. Flesch Readine Ease(FRE), Gunning Fog Index(GFI), SMOG Index(SMOG.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        score {str} : score type {'FRE', 'GFI', SMOG}.\n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        fre {series} : FRE scores.\n",
    "\n",
    "        gfi {series} : GFI scores.\n",
    "\n",
    "        smog {series} : SMOG scores.\n",
    "        \n",
    "        \"\"\"\n",
    "        if(score == 'FRE'):\n",
    "            fre = df['cleaned_review'].apply(textstat.flesch_reading_ease)\n",
    "            return fre\n",
    "        elif(score == 'GFI'):\n",
    "            gfi = df['cleaned_review'].apply(textstat.gunning_fog)\n",
    "            return gfi\n",
    "        elif(score == 'SMOG'):\n",
    "            smog = df['cleaned_review'].apply(textstat.smog_index)\n",
    "            return smog\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported score type. Choose from 'FRE', 'GFI', or 'SMOG'.\")\n",
    "        \n",
    "\n",
    "    def frequency_distribution(self,df,column,fit_transform=False):\n",
    "        \"\"\"\n",
    "        Calculates the word counts in each document.\n",
    "\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        column {str} : Column name for calculating the frequency distribution.\n",
    "        \"\"\"\n",
    "        if fit_transform:\n",
    "            X = self.vectorizer.fit_transform(df[column])\n",
    "        else:\n",
    "            X = self.vectorizer.transform(df[column])\n",
    "        bow = pd.DataFrame(X.toarray(), columns=self.vectorizer.get_feature_names_out())\n",
    "        return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5db0d77-72cf-4b1c-ac61-b831e162d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfe = Statistical_Feature_Engineering()\n",
    "data = sfe.document_statistics(data)\n",
    "data['FRE'] = sfe.readability_score(data)\n",
    "bow = sfe.frequency_distribution(data, column='cleaned_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97128c13-ea78-4921-bf41-293a44e973c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(left=data,right=bow,on=data.index).drop(columns=['key_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5ecbdad-bb2f-4319-be96-374d48513115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_x</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>AWL</th>\n",
       "      <th>ASL</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>UWR</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "      <td>1125</td>\n",
       "      <td>167</td>\n",
       "      <td>10</td>\n",
       "      <td>6.736527</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>140</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>640</td>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>7.619048</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>76</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>580</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>6.823529</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>81</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>446</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>6.757576</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>851</td>\n",
       "      <td>125</td>\n",
       "      <td>9</td>\n",
       "      <td>6.808000</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>101</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_x sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      cleaned_review  char_count  word_count  \\\n",
       "0  one reviewer mentioned watching oz episode you...        1125         167   \n",
       "1  wonderful little production filming technique ...         640          84   \n",
       "2  thought wonderful way spend time hot summer we...         580          85   \n",
       "3  basically there family little boy jake think t...         446          66   \n",
       "4  petter matteis love time money visually stunni...         851         125   \n",
       "\n",
       "   sent_count       AWL        ASL  unique_word_count       UWR  ...  year  \\\n",
       "0          10  6.736527  16.700000                140  0.838323  ...     0   \n",
       "1           7  7.619048  12.000000                 76  0.904762  ...     0   \n",
       "2           4  6.823529  21.250000                 81  0.952941  ...     1   \n",
       "3           6  6.757576  11.000000                 53  0.803030  ...     0   \n",
       "4           9  6.808000  13.888889                101  0.808000  ...     0   \n",
       "\n",
       "   yes  yet  york  youll  young  younger  youre  youve  zombie  \n",
       "0    0    0     0      1      0        0      0      0       0  \n",
       "1    0    0     0      0      0        0      0      0       0  \n",
       "2    0    0     0      0      1        0      0      0       0  \n",
       "3    0    0     0      0      0        0      1      0       1  \n",
       "4    0    0     1      0      0        0      0      0       0  \n",
       "\n",
       "[5 rows x 1011 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1960e874-6ae9-4e8b-9f3e-53af341d88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureSelector:\n",
    "    \"\"\"\n",
    "    A class for implementing various feature selection techniques for text data\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        X           { spmatrix } : Feature matrix\n",
    "        \n",
    "        y           { ndarray }  : Target labels\n",
    "\n",
    "        feature_names { list }   : Names of features\n",
    "        \n",
    "        n_features    { int }    : Number of features to select\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: spmatrix, y: np.ndarray, feature_names: list, n_features: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureSelector with feature matrix and labels\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            X             : Sparse feature matrix\n",
    "            \n",
    "            y             : Target labels\n",
    "            \n",
    "            feature_names : List of feature names\n",
    "            \n",
    "            n_features    : Number of features to select (default: 100% of input features)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError    : If inputs are invalid or incompatible\n",
    "        \"\"\"\n",
    "        if (X.shape[0] != len(y)):\n",
    "            raise ValueError(\"Number of samples in X and y must match\")\n",
    "            \n",
    "        if (X.shape[1] != len(feature_names)):\n",
    "            raise ValueError(\"Number of features must match length of feature_names\")\n",
    "            \n",
    "        self.X             = X\n",
    "        self.y             = y\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features    = n_features or X.shape[1]  # Default 100% of the input features\n",
    "        \n",
    "        \n",
    "    def chi_square_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform chi-square feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Chi-square scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing chi-square feature selection...\")\n",
    "            \n",
    "            # Scale features to non-negative for chi-square\n",
    "            scaler            = MinMaxScaler()\n",
    "            X_scaled          = scaler.fit_transform(self.X.toarray())\n",
    "            \n",
    "            # Apply chi-square selection\n",
    "            selector          = SelectKBest(score_func = chi2, \n",
    "                                            k          = self.n_features)\n",
    "            \n",
    "            selector.fit(X_scaled, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using chi-square\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def information_gain_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform information gain feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Information gain scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing information gain selection...\")\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            selector          = SelectKBest(score_func = mutual_info_classif, \n",
    "                                            k          = self.n_features)\n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using information gain\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def correlation_based_selection(self, threshold: float = 0.8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform correlation-based feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            threshold { float } : Correlation threshold for feature removal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "               { ndarray }      :  Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing correlation-based selection...\")\n",
    "            \n",
    "            # Convert sparse matrix to dense for correlation calculation\n",
    "            X_dense         = self.X.toarray()\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            corr_matrix     = np.corrcoef(X_dense.T)\n",
    "            \n",
    "            # Find highly correlated feature pairs\n",
    "            high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "            \n",
    "            # Keep track of features to remove\n",
    "            to_remove       = set()\n",
    "            \n",
    "            # For each pair of highly correlated features\n",
    "            for i, j in zip(*high_corr_pairs):\n",
    "                if ((i != j) and (i not in to_remove) and (j not in to_remove)):\n",
    "                    # Calculate correlation with target for both features\n",
    "                    corr_i = mutual_info_score(X_dense[:, i], self.y)\n",
    "                    corr_j = mutual_info_score(X_dense[:, j], self.y)\n",
    "                    \n",
    "                    # Remove feature with lower correlation to target\n",
    "                    if (corr_i < corr_j):\n",
    "                        to_remove.add(i)\n",
    "                        \n",
    "                    else:\n",
    "                        to_remove.add(j)\n",
    "            \n",
    "            # Get selected features\n",
    "            all_features      = set(range(self.X.shape[1]))\n",
    "            selected_features = np.array(list(all_features - to_remove))\n",
    "            \n",
    "            # Select top k features if more than n_features remain\n",
    "            if (len(selected_features) > self.n_features):\n",
    "                # Calculate mutual information for remaining features\n",
    "                mi_scores         = mutual_info_classif(self.X[:, selected_features], self.y)\n",
    "                top_k_idx         = np.argsort(mi_scores)[::-1][:self.n_features]\n",
    "                selected_features = selected_features[top_k_idx]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using correlation-based selection\")\n",
    "            \n",
    "            return selected_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def recursive_feature_elimination(self, estimator = None, cv: int = 5) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform Recursive Feature Elimination with cross-validation\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator  : Classifier to use (default: LogisticRegression)\n",
    "\n",
    "            cv         : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple }  : Tuple containing: - Selected feature indices\n",
    "                                           - Feature importance rankings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing recursive feature elimination...\")\n",
    "            \n",
    "            # Use logistic regression if no estimator provided\n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            # Perform RFE with cross-validation\n",
    "            selector = RFECV(estimator              = estimator,\n",
    "                             min_features_to_select = self.n_features,\n",
    "                             cv                     = cv,\n",
    "                             n_jobs                 = -1)\n",
    "            \n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and rankings\n",
    "            selected_features = np.where(selector.support_)[0]\n",
    "            rankings          = selector.ranking_\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using RFE\")\n",
    "            \n",
    "            return selected_features, rankings\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "           \n",
    "        \n",
    "    def forward_selection(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing forward selection...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            selected_features  = list()\n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            for i in tqdm(range(self.n_features)):\n",
    "                best_score   = -np.inf\n",
    "                best_feature = None\n",
    "                \n",
    "                # Try adding each remaining feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = selected_features + [feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score   = avg_score\n",
    "                        best_feature = feature\n",
    "                \n",
    "                if (best_feature is not None):\n",
    "                    selected_features.append(best_feature)\n",
    "                    remaining_features.remove(best_feature)\n",
    "                \n",
    "            print(f\"Selected {len(selected_features)} features using forward selection\")\n",
    "            \n",
    "            return np.array(selected_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def backward_elimination(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward feature elimination\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing backward elimination...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            while len(remaining_features) > self.n_features:\n",
    "                best_score    = -np.inf\n",
    "                worst_feature = None\n",
    "                \n",
    "                # Try removing each feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = [f for f in remaining_features if f != feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores           = cross_val_score(estimator, \n",
    "                                                       X_subset, \n",
    "                                                       self.y,\n",
    "                                                       cv      = cv, \n",
    "                                                       scoring = 'accuracy')\n",
    "                \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score    = avg_score\n",
    "                        worst_feature = feature\n",
    "                \n",
    "                if (worst_feature is not None):\n",
    "                    remaining_features.remove(worst_feature)\n",
    "            \n",
    "            print(f\"Selected {len(remaining_features)} features using backward elimination\")\n",
    "            return np.array(remaining_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c348573-2ba5-4f74-bb56-680f4da90a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['review_x','cleaned_review','sentiment'],axis=1)\n",
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e47c38-57ec-4ba0-b728-0a480c9a6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse = csr_matrix(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4faa639e-fa3b-4cd7-8128-a8cf4dab886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = TextFeatureSelector(X=X_sparse, y=y, feature_names=X.columns.tolist(), n_features=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c064bf7d-eb73-4145-a383-1eed3b69ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing chi-square feature selection...\n",
      "Selected 700 features using chi-square\n"
     ]
    }
   ],
   "source": [
    "selected_features, scores = selector.chi_square_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d446036-8f21-452f-b9d0-9a642457cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,selected_features.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d2d04ec-af2d-465f-ae6e-cc4269e0cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for training and evaluating sentiment analysis models, including testing on unseen data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, feature_eng, selected_feature_indices, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the SentimentAnalyzer by splitting the data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            X                        : Feature matrix (sparse matrix or ndarray)\n",
    "            \n",
    "            y                        : Target labels (array-like)\n",
    "            \n",
    "            feature_eng              : Instance of TextFeatureEngineering\n",
    "            \n",
    "            vectorizers              : Tuple of vectorizers used for feature transformation\n",
    "            \n",
    "            selected_feature_indices : Indices of selected features after feature selection\n",
    "            \n",
    "            test_size                : Proportion of data to use for testing (default: 0.2)\n",
    "            \n",
    "            random_state             : Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size    = test_size, \n",
    "                                                                                random_state = random_state)\n",
    "        \n",
    "        self.feature_eng                                     = feature_eng\n",
    "        #self.vectorizers                                     = vectorizers\n",
    "        self.selected_feature_indices                        = selected_feature_indices\n",
    "\n",
    "        \n",
    "    def train_model(self, model_type:str = \"logistic_regression\", kernel=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train a sentiment analysis model\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model_type { str } : Type of model to train (e.g: \"logistic_regression\", \"svm\", \"random_forest\")\n",
    "            \n",
    "            kernel     { str } : Kernel type for SVM (e.g., \"linear\", \"poly\", \"rbf\", \"sigmoid\")\n",
    "            \n",
    "            kwargs             : Additional arguments for the model initialization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        if (model_type == \"logistic_regression\"):\n",
    "            model = LogisticRegression(max_iter = 1000, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"svm\"):\n",
    "            \n",
    "            if (kernel is None):\n",
    "                # Default kernel\n",
    "                kernel = \"rbf\"  \n",
    "                \n",
    "            model = SVC(kernel = kernel, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"random_forest\"):\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "            \n",
    "        elif model_type == \"naive_bayes\":\n",
    "            model = MultinomialNB(**kwargs)\n",
    "\n",
    "        elif model_type == \"lightgbm\":\n",
    "            model = LGBMClassifier(**kwargs)\n",
    "\n",
    "        elif model_type == \"logistic_model_tree\":\n",
    "            # Create a logistic regression model\n",
    "            logistic_model      = LogisticRegression(max_iter = 1000, **kwargs)\n",
    "\n",
    "            # Create a decision tree model\n",
    "            decision_tree_model = DecisionTreeClassifier(**kwargs)\n",
    "\n",
    "            # Combine them in a stacking model\n",
    "            model               = StackingClassifier(estimators      = [('decision_tree', decision_tree_model)], \n",
    "                                                     final_estimator = logistic_model, \n",
    "                                                     **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose from : 'logistic_regression', 'svm', 'random_forest', 'naive_bayes', 'lightgbm', 'logistic_model_tree'\")\n",
    "\n",
    "        print(f\"Training {model_type}...\")\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model on the test set\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model : Trained model\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred   = model.predict(self.X_test)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report   = classification_report(self.y_test, y_pred)\n",
    "        cm       = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\"accuracy\"              : accuracy,\n",
    "                \"classification_report\" : report,\n",
    "                \"confusion_matrix\"      : cm,\n",
    "               }\n",
    "\n",
    "    \n",
    "    def test_on_unseen_data(self, model, unseen_texts, unseen_labels=None, **preprocessed_features):\n",
    "        \"\"\"\n",
    "        Test the model on unseen data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model                 : Trained model\n",
    "            \n",
    "            unseen_texts          : List of unseen text data\n",
    "\n",
    "            unseen_labels         : True labels for the unseen data\n",
    "\n",
    "            preprocessed_features : Preprocessed feature matrices (e.g., binary_features, tfidf_features, bm25_features, etc.)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Predictions for the unseen data\n",
    "        \"\"\"\n",
    "        print(\"Processing unseen data...\")\n",
    "\n",
    "        # Dynamically combine all passed feature matrices\n",
    "        unseen_combined_features = hstack([preprocessed_features[key] for key in preprocessed_features])\n",
    "\n",
    "        # Select features using the indices chosen during feature selection\n",
    "        unseen_selected_features = unseen_combined_features[:, self.selected_feature_indices]\n",
    "\n",
    "        # Predict sentiments\n",
    "        predictions              = model.predict(unseen_selected_features)\n",
    "\n",
    "        # Print predictions\n",
    "        print(\"Predictions on Unseen Data:\")\n",
    "        for text, pred in zip(unseen_texts, predictions):\n",
    "            print(f\"Text: {text}\\nPredicted Sentiment: {pred}\\n\")\n",
    "\n",
    "        # Compute accuracy if unseen_labels are provided\n",
    "        if unseen_labels is not None:\n",
    "            print(f\"Number of unseen_labels: {len(unseen_labels)}\")\n",
    "\n",
    "            if (len(unseen_labels) != len(predictions)):\n",
    "                raise ValueError(\"The number of unseen_labels must match the number of predictions.\")\n",
    "                \n",
    "            accuracy = accuracy_score(unseen_labels, predictions)\n",
    "            print(f\"Accuracy on Unseen Data : {accuracy:.4f}\")\n",
    "            return predictions, accuracy\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "53bdb40d-abce-4a0a-9458-d3119fe6e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer                            = SentimentAnalyzer(X                        = X, \n",
    "                                                                  y                        = y,\n",
    "                                                                  feature_eng              = sfe,\n",
    "                                                                  selected_feature_indices = selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4bf5e44e-edc9-45bb-8d9e-cf319e5e0171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic_regression...\n"
     ]
    }
   ],
   "source": [
    "logistic_model                                = sentiment_analyzer.train_model(model_type = \"logistic_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fc7d206-5d06-4c03-bd23-2be833e969c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Accuracy: 0.8588\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.85      0.86      4961\n",
      "    positive       0.85      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4194  767]\n",
      " [ 645 4394]]\n"
     ]
    }
   ],
   "source": [
    "evaluation_results                            = sentiment_analyzer.evaluate_model(logistic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0312a5b-aac8-4fcb-8ceb-982bb1fe058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('~/Project/NLP_Tasks/data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c453e710-14bd-4a6a-a896-68b24e3d47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.rename(columns={'Text':'review','Sentiment':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b09284b-6479-41db-aef6-58acdfc68baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "test_data['cleaned_review'] = test_data['review'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9fcb2c7a-7fab-4078-b1c7-52f7c2670881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is speedy enough with plot twists, ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>movie speedy enough plot twist hard understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seriously, this is the best movie I've ever wa...</td>\n",
       "      <td>positive</td>\n",
       "      <td>seriously best movie ive ever watched everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The storyline was okay, but the acting was jus...</td>\n",
       "      <td>positive</td>\n",
       "      <td>storyline okay acting mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A complete disaster of a movie. Don't waste yo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complete disaster movie dont waste time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can't believe how amazing this was. Totally ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>cant believe amazing totally worth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This movie is speedy enough with plot twists, ...  negative   \n",
       "1  Seriously, this is the best movie I've ever wa...  positive   \n",
       "2  The storyline was okay, but the acting was jus...  positive   \n",
       "3  A complete disaster of a movie. Don't waste yo...  negative   \n",
       "4  I can't believe how amazing this was. Totally ...  positive   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  movie speedy enough plot twist hard understand...  \n",
       "1  seriously best movie ive ever watched everythi...  \n",
       "2                         storyline okay acting mark  \n",
       "3            complete disaster movie dont waste time  \n",
       "4                 cant believe amazing totally worth  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e552e094-1480-492e-9e89-2e92b5520977",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfe = Statistical_Feature_Engineering()\n",
    "test_data = sfe.document_statistics(test_data)\n",
    "test_data['FRE'] = sfe.readability_score(test_data)\n",
    "bow = sfe.frequency_distribution(test_data, column='cleaned_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b930f511-4a7b-4665-8ce6-6e214cd8045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(left=test_data,right=bow,on=test_data.index).drop(columns=['key_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "723bd7a4-b83d-4f6c-aae2-0087d39f46f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>AWL</th>\n",
       "      <th>ASL</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>UWR</th>\n",
       "      <th>...</th>\n",
       "      <th>watched</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>weekend</th>\n",
       "      <th>well</th>\n",
       "      <th>worth</th>\n",
       "      <th>wow</th>\n",
       "      <th>year</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is speedy enough with plot twists, ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>movie speedy enough plot twist hard understand...</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seriously, this is the best movie I've ever wa...</td>\n",
       "      <td>positive</td>\n",
       "      <td>seriously best movie ive ever watched everythi...</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The storyline was okay, but the acting was jus...</td>\n",
       "      <td>positive</td>\n",
       "      <td>storyline okay acting mark</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A complete disaster of a movie. Don't waste yo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>complete disaster movie dont waste time</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can't believe how amazing this was. Totally ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>cant believe amazing totally worth</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This movie is speedy enough with plot twists, ...  negative   \n",
       "1  Seriously, this is the best movie I've ever wa...  positive   \n",
       "2  The storyline was okay, but the acting was jus...  positive   \n",
       "3  A complete disaster of a movie. Don't waste yo...  negative   \n",
       "4  I can't believe how amazing this was. Totally ...  positive   \n",
       "\n",
       "                                      cleaned_review  char_count  word_count  \\\n",
       "0  movie speedy enough plot twist hard understand...          62           9   \n",
       "1  seriously best movie ive ever watched everythi...          57           8   \n",
       "2                         storyline okay acting mark          26           4   \n",
       "3            complete disaster movie dont waste time          39           6   \n",
       "4                 cant believe amazing totally worth          34           5   \n",
       "\n",
       "   sent_count       AWL  ASL  unique_word_count       UWR  ...  watched  \\\n",
       "0           1  6.888889  9.0                  8  0.888889  ...        0   \n",
       "1           2  7.125000  4.0                  8  1.000000  ...        1   \n",
       "2           1  6.500000  4.0                  4  1.000000  ...        0   \n",
       "3           2  6.500000  3.0                  6  1.000000  ...        0   \n",
       "4           2  6.800000  2.5                  5  1.000000  ...        0   \n",
       "\n",
       "   watching  way  weekend  well  worth  wow  year  youll  youre  \n",
       "0         0    0        0     0      0    0     0      0      0  \n",
       "1         0    0        0     0      0    0     0      0      0  \n",
       "2         0    0        0     0      0    0     0      0      0  \n",
       "3         0    0        0     0      0    0     0      0      0  \n",
       "4         0    0        0     0      1    0     0      0      0  \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b01e6c8-3502-4af2-ad2e-460245e0173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(columns=['review','sentiment','cleaned_review'],axis=1)\n",
    "X_test = csr_matrix(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d384f78c-0752-47e5-b38a-1b4cd8017061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing unseen data...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (1007) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentiment_analyzer\u001b[38;5;241m.\u001b[39mtest_on_unseen_data(model\u001b[38;5;241m=\u001b[39mlogistic_model,\n\u001b[1;32m      2\u001b[0m                                       unseen_texts\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                       unseen_labels\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                       statistical_features \u001b[38;5;241m=\u001b[39m X_test)\n",
      "Cell \u001b[0;32mIn[104], line 147\u001b[0m, in \u001b[0;36mSentimentAnalyzer.test_on_unseen_data\u001b[0;34m(self, model, unseen_texts, unseen_labels, **preprocessed_features)\u001b[0m\n\u001b[1;32m    144\u001b[0m unseen_combined_features \u001b[38;5;241m=\u001b[39m hstack([preprocessed_features[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m preprocessed_features])\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Select features using the indices chosen during feature selection\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m unseen_selected_features \u001b[38;5;241m=\u001b[39m unseen_combined_features[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_indices]\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Predict sentiments\u001b[39;00m\n\u001b[1;32m    150\u001b[0m predictions              \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(unseen_selected_features)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/_index.py:46\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m---> 46\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_indices(key)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/_index.py:167\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    165\u001b[0m         col \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asindices(col, N)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row, col\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/sparse/_index.py:190\u001b[0m, in \u001b[0;36mIndexMixin._asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    188\u001b[0m max_indx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_indx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m length:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) out of range\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m max_indx)\n\u001b[1;32m    192\u001b[0m min_indx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_indx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index (1007) out of range"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer.test_on_unseen_data(model=logistic_model,\n",
    "                                      unseen_texts=test_data['review'],\n",
    "                                      unseen_labels=test_data['sentiment'],\n",
    "                                      statistical_features = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c1718a9d-165f-42e7-b689-0351ebb3fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "train_data = pd.read_csv(\"~/Project/NLP_Tasks/data/IMDB_Dataset.csv\") \n",
    "test_data = pd.read_csv('~/Project/NLP_Tasks/data/test_data.csv')\n",
    "test_data = test_data.rename(columns={'Text':'review','Sentiment':'sentiment'})\n",
    "\n",
    "# TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "train_data['cleaned_review'] = train_data['review'].apply(text_preprocessor.clean_text)\n",
    "test_data['cleaned_review'] = test_data['review'].apply(text_preprocessor.clean_text)\n",
    "\n",
    "#Statistical_Feature_Engineering\n",
    "feature_engineer = Statistical_Feature_Engineering()\n",
    "\n",
    "#document_statistics\n",
    "train_data  = feature_engineer.document_statistics(train_data)\n",
    "test_data  = feature_engineer.document_statistics(test_data)\n",
    "\n",
    "#frequency_distribution\n",
    "train_bow = feature_engineer.frequency_distribution(train_data, 'cleaned_review',fit_transform=True)\n",
    "test_bow = feature_engineer.frequency_distribution(test_data, 'cleaned_review',fit_transform=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b91d7-944e-43a6-84ff-dfbfb5393359",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "70793160-5cc6-4fba-a06b-15c8a70ce846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing chi-square feature selection...\n",
      "Selected 900 features using chi-square\n",
      "Training logistic_regression...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8597\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      4961\n",
      "           1       0.85      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4189  772]\n",
      " [ 631 4408]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "stat_features = train_data[['char_count', 'word_count', 'sent_count', 'AWL', 'ASL', 'unique_word_count', 'UWR']].values\n",
    "stat_features_sparse = csr_matrix(stat_features)\n",
    "train_bow_sparse = csr_matrix(train_bow)\n",
    "X_combined = hstack([stat_features_sparse, train_bow_sparse])\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_data['sentiment'])\n",
    "\n",
    "#Feature Selection\n",
    "selector = TextFeatureSelector(X_combined, y, feature_names=list(train_bow.columns) + list(train_data.columns[-7:]), n_features=900)\n",
    "selected_features, chi2_scores = selector.chi_square_selection()\n",
    "X_combined_csr = csr_matrix(X_combined)\n",
    "X_selected = X_combined_csr[:, selected_features]\n",
    "\n",
    "# Model Training\n",
    "sentiment_analyzer = SentimentAnalyzer(X_selected, y, feature_engineer, selected_feature_indices=selected_features)\n",
    "model = sentiment_analyzer.train_model(model_type=\"logistic_regression\")\n",
    "\n",
    "# Model Evaluation\n",
    "metrics = sentiment_analyzer.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056934c-5592-42af-be8e-59dfe436e9f2",
   "metadata": {},
   "source": [
    "# Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bbcd2af5-8579-4fc9-a162-4feb774e842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing unseen data...\n",
      "Predictions on Unseen Data:\n",
      "Text: This movie is speedy enough with plot twists, but hard to understand the connection between plots.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Seriously, this is the best movie I've ever watched! Everything was flawless!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The storyline was okay, but the acting was just not up to the mark.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A complete disaster of a movie. Don't waste your time.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: I can't believe how amazing this was. Totally worth it!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The movie had its moments, but overall, it felt like something was missing.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: I absolutely loved the cinematography, but the acting was subpar.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The film is an excellent example of how not to make a movie.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: It's hard to imagine how anyone could dislike this masterpiece!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The trailer was better than the actual movie. Felt cheated.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A rollercoaster of emotions! Highly recommend watching this.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: An average movie with nothing new to offer.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: The pacing was terrible, and the climax was predictable.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Wow, just wow. This is how a movie should be made!\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A decent watch for a lazy weekend. Not groundbreaking, but enjoyable.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The director has outdone themselves; what a phenomenal movie!\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: More hype than substance. A complete letdown.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Good visuals, decent music, but lacked a solid script.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A masterpiece in every sense. This will stay with me forever.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Mediocre at best. Not worth the ticket price.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: A fresh take on a tired genre. Highly recommend it!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Overrated and boring. Nothing special about it.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: This is one of those movies you'll regret missing. A must-watch!\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Predictable plot, but the performances were top-notch.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: It's a bad movie if you're looking for entertainment.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Can't believe I sat through the entire thing. A waste of time.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: Finally, a movie that gets it right. Loved every minute of it!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: A forgettable movie with no real impact.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: An extraordinary journey that left me speechless. Bravo!\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: The humor was forced, and the dialogue was cringeworthy.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A solid movie with a gripping narrative. Well done!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The music was fantastic, but the rest of the movie was average.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Ironic how they managed to make something so beautiful look so bland.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: An epic conclusion to a fantastic series. Couldn’t have been better!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The movie tries too hard to be funny and fails miserably.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A fresh and engaging story with relatable characters.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: All style, no substance. Disappointing.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: A breath of fresh air! One of the best movies this year.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The plot was all over the place, but it was fun to watch.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Couldn't make it through the first half. Painful to sit through.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: An unexpectedly beautiful film that touched my heart.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Trying to understand why this movie exists is more entertaining than the movie itself.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: Every second of this movie was a blessing. Pure cinematic joy.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: The lead actor was the only saving grace in an otherwise dull film.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: A pretentious attempt at storytelling that falls flat.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: I didn’t expect much, but this movie surprised me in the best way.\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: A series of poorly executed clichés masquerading as a story.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Text: This is not just a movie; it’s an experience. Brilliant!\n",
      "Predicted Sentiment: 1\n",
      "\n",
      "Text: A slog of a movie with a laughably bad ending.\n",
      "Predicted Sentiment: 0\n",
      "\n",
      "Number of unseen_labels: 49\n",
      "Accuracy on Unseen Data : 0.6735\n",
      "Accuracy on Test Data: 0.6735\n",
      "\n",
      "Predictions:\n",
      "[0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_stat_features = test_data[['char_count', 'word_count', 'sent_count', 'AWL', 'ASL', 'unique_word_count', 'UWR']].values\n",
    "test_stat_features_sparse = csr_matrix(test_stat_features)\n",
    "test_bow_sparse = csr_matrix(test_bow)\n",
    "test_combined = hstack([test_stat_features_sparse, test_bow_sparse])\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "test_data['sentiment'] = label_encoder.fit_transform(test_data['sentiment'])\n",
    "\n",
    "# test_on_unseen_data\n",
    "predictions, accuracy = sentiment_analyzer.test_on_unseen_data(\n",
    "    model, \n",
    "    unseen_texts=test_data['review'].tolist(),  # List of the raw review texts\n",
    "    unseen_labels=test_data['sentiment'].values,  # Actual sentiment labels, if available\n",
    "    statistical_features = test_stat_features_sparse,\n",
    "    bow_feature = test_bow_sparse\n",
    ")\n",
    "\n",
    "print(f\"Accuracy on Test Data: {accuracy:.4f}\")\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17fd44-9383-4733-a956-2d5936e69dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
