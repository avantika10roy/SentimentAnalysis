{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d335efc2-ebda-4525-99fb-05d2b130b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spmatrix\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore all runtime warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d6554b-80a8-410f-8ed1-f905f5633d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/it012314/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/it012314/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/it012314/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/it012314/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading Packages from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c605c-7df9-4016-9e5b-ca7ad90672ba",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289b6817-47a8-4d30-b1f8-8257b1ee8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "imdb_data = pd.read_csv('../data/IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ebdc3b-4063-48f2-bb99-893e5f9cd49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Preview\n",
    "imdb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79117a06-aae7-4970-9e9f-8834512211a2",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06720b01-3342-4862-9a21-e1855d2e2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data through cleaning, tokenization, and normalization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        lemmatizer : WordNetLemmatizer instance for word lemmatization\n",
    "        \n",
    "        stop_words : Set of stopwords to be removed from text\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TextPreprocessor with required NLTK resources\n",
    "        \n",
    "        Raises:\n",
    "        -------\n",
    "            LookupError : If required NLTK resources cannot be downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            \n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        except LookupError as e:\n",
    "            raise\n",
    "    \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text by removing HTML tags, special characters,\n",
    "        and applying text normalization techniques\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            text { str }      : Input text to be cleaned\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError        : If input text is None or empty\n",
    "            \n",
    "            TextCleaningError : If any error occurs at any step of text cleaning process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "                { str }       : Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if ((not text) or (not isinstance(text, str))):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text   = re.sub('<[^>]*>', '', text)\n",
    "            \n",
    "            # Remove special characters and digits\n",
    "            text   = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            text   = text.lower()\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        except Exception as TextCleaningError:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679c0f8f-552d-4c2b-a544-ff456953ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor                  = TextPreprocessor()\n",
    "\n",
    "# Add a new column to the original DataFrame to store the cleaned texts\n",
    "imdb_data['clean_text'] = imdb_data['review'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8450720f-8415-416c-8fc9-ef16f1cb5821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>probably alltime favorite movie story selfless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>show amazing fresh innovative idea first aired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>encouraged positive comment film looking forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>like original gut wrenching laughter like movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "5  Probably my all-time favorite movie, a story o...  positive   \n",
       "6  I sure would like to see a resurrection of a u...  positive   \n",
       "7  This show was an amazing, fresh & innovative i...  negative   \n",
       "8  Encouraged by the positive comments about this...  negative   \n",
       "9  If you like original gut wrenching laughter yo...  positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0  one reviewer mentioned watching oz episode you...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  \n",
       "5  probably alltime favorite movie story selfless...  \n",
       "6  sure would like see resurrection dated seahunt...  \n",
       "7  show amazing fresh innovative idea first aired...  \n",
       "8  encouraged positive comment film looking forwa...  \n",
       "9  like original gut wrenching laughter like movi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings After Cleaning\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3ab36-e3bb-4b8a-9c6a-f605eb7ed55a",
   "metadata": {},
   "source": [
    "## Contextual Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19b7269-78dc-4fc7-b819-ed2339742385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contextual_Features:\n",
    "    \"\"\"\n",
    "    A class for implementing various text feature engineering techniques\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create\n",
    "        \n",
    "        ngram_range  { tuple } : Range of n-grams to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None, ngram_range: tuple = (1, 3)) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureEngineering with texts and parameters\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit)\n",
    "            \n",
    "            ngram_range  : Range of n-grams to consider (min_n, max_n)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range  = ngram_range\n",
    "        \n",
    "    def window_based(self):\n",
    "        \"\"\"\n",
    "        Create Window Based Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts             : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Window-Based Contextual Features:...\")\n",
    "            vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            ngrams_features     = vectorizer.fit_transform(self.texts)\n",
    "            \n",
    "            return vectorizer, ngrams_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def position_based(self):\n",
    "        \"\"\"\n",
    "        Create Position Based Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts              : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Position-Based Contextual Features:...\")\n",
    "            position_features = []\n",
    "            \n",
    "            for doc in self.texts:\n",
    "                words = doc.split() \n",
    "            \n",
    "                position_features.extend([{\"word\": word, \"position\": idx} for idx, word in enumerate(words)])\n",
    "\n",
    "            return position_features\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def generate_ngrams(self, n=3):\n",
    "        \"\"\"\n",
    "        Generate N-Grams\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        words         : List of words taken individually from the preprocessed text documents\n",
    "        n             : Individual words from the list\n",
    "        \"\"\"\n",
    "        print(\"Generating N-Grams:...\")\n",
    "        ngrams = []\n",
    "\n",
    "        for doc in self.texts:\n",
    "            words = doc.split() \n",
    "            ngrams.extend([tuple(words[i:i+n]) for i in range(len(words)-n+1)]) \n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def cross_document(self):\n",
    "        \"\"\"\n",
    "        Create Cross Document Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts             : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try: \n",
    "            print(\"Creating Cross Document Contextual Feature Engineering:...\")\n",
    "            vectorizer   = TfidfVectorizer(max_features = self.max_features, \n",
    "                                           ngram_range  = self.ngram_range)\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.texts)\n",
    "            return vectorizer, tfidf_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86fc8d56-3f30-4c2f-b84f-95d6999752fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextuals = Contextual_Features(texts        = imdb_data['clean_text'].tolist(),\n",
    "                                  max_features = 100,\n",
    "                                  ngram_range  = (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5363c6d-84f3-49f4-8081-657b9368ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Window-Based Contextual Features:...\n",
      "Creating Position-Based Contextual Features:...\n",
      "Generating N-Grams:...\n",
      "Creating Cross Document Contextual Feature Engineering:...\n"
     ]
    }
   ],
   "source": [
    "window_vectorizer, window_features = contextuals.window_based()\n",
    "positional_features                = contextuals.position_based()\n",
    "trigrams                           = contextuals.generate_ngrams()\n",
    "cross_doc_vectorizer, tfidf_matrix = contextuals.cross_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e19946d-09b7-48dc-b9dc-130230de3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'worth watching': np.int64(95), 'well done': np.int64(91), 'make film': np.int64(43), 'real life': np.int64(75), 'new york': np.int64(64), 'would like': np.int64(96), 'waste time': np.int64(87), 'ive seen': np.int64(35), 'one worst': np.int64(69), 'like movie': np.int64(36), 'low budget': np.int64(41), 'film would': np.int64(19), 'saw movie': np.int64(78), 'year old': np.int64(99), 'year later': np.int64(98), 'see movie': np.int64(80), 'good thing': np.int64(26), 'ive ever': np.int64(34), 'ever seen': np.int64(9), 'high school': np.int64(30), 'special effect': np.int64(83), 'movie made': np.int64(55), 'story line': np.int64(84), 'movie could': np.int64(49), 'first film': np.int64(20), 'film also': np.int64(13), 'film like': np.int64(15), 'film one': np.int64(17), 'look like': np.int64(39), 'bad guy': np.int64(1), 'good movie': np.int64(25), 'seems like': np.int64(81), 'pretty much': np.int64(72), 'worst movie': np.int64(94), 'much better': np.int64(61), 'main character': np.int64(42), 'feel like': np.int64(12), 'like one': np.int64(37), 'bad acting': np.int64(0), 'movie movie': np.int64(57), 'many time': np.int64(47), 'love story': np.int64(40), 'even though': np.int64(7), 'film ever': np.int64(14), 'ever made': np.int64(8), 'one best': np.int64(65), 'whole thing': np.int64(93), 'take place': np.int64(85), 'film made': np.int64(16), 'film really': np.int64(18), 'watch movie': np.int64(89), 'movie ive': np.int64(53), 'watching movie': np.int64(90), 'seen movie': np.int64(82), 'part movie': np.int64(70), 'whole movie': np.int64(92), 'see film': np.int64(79), 'many people': np.int64(46), 'watch film': np.int64(88), 'want see': np.int64(86), 'movie make': np.int64(56), 'movie would': np.int64(60), 'dont think': np.int64(6), 'im sure': np.int64(33), 'movie like': np.int64(54), 'movie ever': np.int64(51), 'movie really': np.int64(59), 'movie even': np.int64(50), 'great movie': np.int64(28), 'bad movie': np.int64(2), 'never seen': np.int64(63), 'dont know': np.int64(5), 'horror movie': np.int64(32), 'pretty good': np.int64(71), 'first movie': np.int64(21), 'make movie': np.int64(44), 'make sense': np.int64(45), 'read book': np.int64(74), 'good job': np.int64(24), 'one thing': np.int64(68), 'movie one': np.int64(58), 'horror film': np.int64(31), 'fall love': np.int64(11), 'really good': np.int64(77), 'great film': np.int64(27), 'good film': np.int64(23), 'movie good': np.int64(52), 'one film': np.int64(66), 'every time': np.int64(10), 'first time': np.int64(22), 'come across': np.int64(3), 'year ago': np.int64(97), 'movie bad': np.int64(48), 'must see': np.int64(62), 'really bad': np.int64(76), 'one movie': np.int64(67), 'production value': np.int64(73), 'dont get': np.int64(4), 'long time': np.int64(38), 'havent seen': np.int64(29)}\n",
      "N-Gram Features:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\\n\", window_vectorizer.vocabulary_)\n",
    "print(\"N-Gram Features:\\n\", window_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f064e6b9-974d-4608-a9cc-d415dcc8db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Features:\n",
      "               word  position\n",
      "0              one         0\n",
      "1         reviewer         1\n",
      "2        mentioned         2\n",
      "3         watching         3\n",
      "4               oz         4\n",
      "...            ...       ...\n",
      "5928987       even        63\n",
      "5928988      cable        64\n",
      "5928989    channel        65\n",
      "5928990      avoid        66\n",
      "5928991      movie        67\n",
      "\n",
      "[5928992 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Positional Features:\\n\", pd.DataFrame(positional_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb48cfec-934f-4541-b823-45c528a9ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams:\n",
      " ('one', 'reviewer', 'mentioned')\n",
      "Trigrams:\n",
      " ('reviewer', 'mentioned', 'watching')\n",
      "Trigrams:\n",
      " ('mentioned', 'watching', 'oz')\n",
      "Trigrams:\n",
      " ('watching', 'oz', 'episode')\n",
      "Trigrams:\n",
      " ('oz', 'episode', 'youll')\n",
      "Trigrams:\n",
      " ('episode', 'youll', 'hooked')\n",
      "Trigrams:\n",
      " ('youll', 'hooked', 'right')\n",
      "Trigrams:\n",
      " ('hooked', 'right', 'exactly')\n",
      "Trigrams:\n",
      " ('right', 'exactly', 'happened')\n",
      "Trigrams:\n",
      " ('exactly', 'happened', 'methe')\n",
      "Trigrams:\n",
      " ('happened', 'methe', 'first')\n",
      "Trigrams:\n",
      " ('methe', 'first', 'thing')\n",
      "Trigrams:\n",
      " ('first', 'thing', 'struck')\n",
      "Trigrams:\n",
      " ('thing', 'struck', 'oz')\n",
      "Trigrams:\n",
      " ('struck', 'oz', 'brutality')\n",
      "Trigrams:\n",
      " ('oz', 'brutality', 'unflinching')\n",
      "Trigrams:\n",
      " ('brutality', 'unflinching', 'scene')\n",
      "Trigrams:\n",
      " ('unflinching', 'scene', 'violence')\n",
      "Trigrams:\n",
      " ('scene', 'violence', 'set')\n",
      "Trigrams:\n",
      " ('violence', 'set', 'right')\n",
      "Trigrams:\n",
      " ('set', 'right', 'word')\n",
      "Trigrams:\n",
      " ('right', 'word', 'go')\n",
      "Trigrams:\n",
      " ('word', 'go', 'trust')\n",
      "Trigrams:\n",
      " ('go', 'trust', 'show')\n",
      "Trigrams:\n",
      " ('trust', 'show', 'faint')\n",
      "Trigrams:\n",
      " ('show', 'faint', 'hearted')\n",
      "Trigrams:\n",
      " ('faint', 'hearted', 'timid')\n",
      "Trigrams:\n",
      " ('hearted', 'timid', 'show')\n",
      "Trigrams:\n",
      " ('timid', 'show', 'pull')\n",
      "Trigrams:\n",
      " ('show', 'pull', 'punch')\n",
      "Trigrams:\n",
      " ('pull', 'punch', 'regard')\n",
      "Trigrams:\n",
      " ('punch', 'regard', 'drug')\n",
      "Trigrams:\n",
      " ('regard', 'drug', 'sex')\n",
      "Trigrams:\n",
      " ('drug', 'sex', 'violence')\n",
      "Trigrams:\n",
      " ('sex', 'violence', 'hardcore')\n",
      "Trigrams:\n",
      " ('violence', 'hardcore', 'classic')\n",
      "Trigrams:\n",
      " ('hardcore', 'classic', 'use')\n",
      "Trigrams:\n",
      " ('classic', 'use', 'wordit')\n",
      "Trigrams:\n",
      " ('use', 'wordit', 'called')\n",
      "Trigrams:\n",
      " ('wordit', 'called', 'oz')\n",
      "Trigrams:\n",
      " ('called', 'oz', 'nickname')\n",
      "Trigrams:\n",
      " ('oz', 'nickname', 'given')\n",
      "Trigrams:\n",
      " ('nickname', 'given', 'oswald')\n",
      "Trigrams:\n",
      " ('given', 'oswald', 'maximum')\n",
      "Trigrams:\n",
      " ('oswald', 'maximum', 'security')\n",
      "Trigrams:\n",
      " ('maximum', 'security', 'state')\n",
      "Trigrams:\n",
      " ('security', 'state', 'penitentary')\n",
      "Trigrams:\n",
      " ('state', 'penitentary', 'focus')\n",
      "Trigrams:\n",
      " ('penitentary', 'focus', 'mainly')\n",
      "Trigrams:\n",
      " ('focus', 'mainly', 'emerald')\n",
      "Trigrams:\n",
      " ('mainly', 'emerald', 'city')\n",
      "Trigrams:\n",
      " ('emerald', 'city', 'experimental')\n",
      "Trigrams:\n",
      " ('city', 'experimental', 'section')\n",
      "Trigrams:\n",
      " ('experimental', 'section', 'prison')\n",
      "Trigrams:\n",
      " ('section', 'prison', 'cell')\n",
      "Trigrams:\n",
      " ('prison', 'cell', 'glass')\n",
      "Trigrams:\n",
      " ('cell', 'glass', 'front')\n",
      "Trigrams:\n",
      " ('glass', 'front', 'face')\n",
      "Trigrams:\n",
      " ('front', 'face', 'inwards')\n",
      "Trigrams:\n",
      " ('face', 'inwards', 'privacy')\n",
      "Trigrams:\n",
      " ('inwards', 'privacy', 'high')\n",
      "Trigrams:\n",
      " ('privacy', 'high', 'agenda')\n",
      "Trigrams:\n",
      " ('high', 'agenda', 'em')\n",
      "Trigrams:\n",
      " ('agenda', 'em', 'city')\n",
      "Trigrams:\n",
      " ('em', 'city', 'home')\n",
      "Trigrams:\n",
      " ('city', 'home', 'manyaryans')\n",
      "Trigrams:\n",
      " ('home', 'manyaryans', 'muslim')\n",
      "Trigrams:\n",
      " ('manyaryans', 'muslim', 'gangsta')\n",
      "Trigrams:\n",
      " ('muslim', 'gangsta', 'latino')\n",
      "Trigrams:\n",
      " ('gangsta', 'latino', 'christian')\n",
      "Trigrams:\n",
      " ('latino', 'christian', 'italian')\n",
      "Trigrams:\n",
      " ('christian', 'italian', 'irish')\n",
      "Trigrams:\n",
      " ('italian', 'irish', 'moreso')\n",
      "Trigrams:\n",
      " ('irish', 'moreso', 'scuffle')\n",
      "Trigrams:\n",
      " ('moreso', 'scuffle', 'death')\n",
      "Trigrams:\n",
      " ('scuffle', 'death', 'stare')\n",
      "Trigrams:\n",
      " ('death', 'stare', 'dodgy')\n",
      "Trigrams:\n",
      " ('stare', 'dodgy', 'dealing')\n",
      "Trigrams:\n",
      " ('dodgy', 'dealing', 'shady')\n",
      "Trigrams:\n",
      " ('dealing', 'shady', 'agreement')\n",
      "Trigrams:\n",
      " ('shady', 'agreement', 'never')\n",
      "Trigrams:\n",
      " ('agreement', 'never', 'far')\n",
      "Trigrams:\n",
      " ('never', 'far', 'awayi')\n",
      "Trigrams:\n",
      " ('far', 'awayi', 'would')\n",
      "Trigrams:\n",
      " ('awayi', 'would', 'say')\n",
      "Trigrams:\n",
      " ('would', 'say', 'main')\n",
      "Trigrams:\n",
      " ('say', 'main', 'appeal')\n",
      "Trigrams:\n",
      " ('main', 'appeal', 'show')\n",
      "Trigrams:\n",
      " ('appeal', 'show', 'due')\n",
      "Trigrams:\n",
      " ('show', 'due', 'fact')\n",
      "Trigrams:\n",
      " ('due', 'fact', 'go')\n",
      "Trigrams:\n",
      " ('fact', 'go', 'show')\n",
      "Trigrams:\n",
      " ('go', 'show', 'wouldnt')\n",
      "Trigrams:\n",
      " ('show', 'wouldnt', 'dare')\n",
      "Trigrams:\n",
      " ('wouldnt', 'dare', 'forget')\n",
      "Trigrams:\n",
      " ('dare', 'forget', 'pretty')\n",
      "Trigrams:\n",
      " ('forget', 'pretty', 'picture')\n",
      "Trigrams:\n",
      " ('pretty', 'picture', 'painted')\n",
      "Trigrams:\n",
      " ('picture', 'painted', 'mainstream')\n",
      "Trigrams:\n",
      " ('painted', 'mainstream', 'audience')\n"
     ]
    }
   ],
   "source": [
    "for i in range (100):\n",
    "    print(\"Trigrams:\\n\", trigrams[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d7ae76-16b6-464c-8d72-d7ff00152f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer:\n",
      " ['bad acting' 'bad guy' 'bad movie' 'come across' 'dont get' 'dont know'\n",
      " 'dont think' 'even though' 'ever made' 'ever seen' 'every time'\n",
      " 'fall love' 'feel like' 'film also' 'film ever' 'film like' 'film made'\n",
      " 'film one' 'film really' 'film would' 'first film' 'first movie'\n",
      " 'first time' 'good film' 'good job' 'good movie' 'good thing'\n",
      " 'great film' 'great movie' 'havent seen' 'high school' 'horror film'\n",
      " 'horror movie' 'im sure' 'ive ever' 'ive seen' 'like movie' 'like one'\n",
      " 'long time' 'look like' 'love story' 'low budget' 'main character'\n",
      " 'make film' 'make movie' 'make sense' 'many people' 'many time'\n",
      " 'movie bad' 'movie could' 'movie even' 'movie ever' 'movie good'\n",
      " 'movie ive' 'movie like' 'movie made' 'movie make' 'movie movie'\n",
      " 'movie one' 'movie really' 'movie would' 'much better' 'must see'\n",
      " 'never seen' 'new york' 'one best' 'one film' 'one movie' 'one thing'\n",
      " 'one worst' 'part movie' 'pretty good' 'pretty much' 'production value'\n",
      " 'read book' 'real life' 'really bad' 'really good' 'saw movie' 'see film'\n",
      " 'see movie' 'seems like' 'seen movie' 'special effect' 'story line'\n",
      " 'take place' 'want see' 'waste time' 'watch film' 'watch movie'\n",
      " 'watching movie' 'well done' 'whole movie' 'whole thing' 'worst movie'\n",
      " 'worth watching' 'would like' 'year ago' 'year later' 'year old']\n",
      "TF-IDF Matrix [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Vectorizer:\\n\", cross_doc_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4d439e-5e5d-433c-90b4-17a1f1ff71ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "wb = window_features.toarray()\n",
    "print(wb[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bdac6a1-efa0-4500-99cc-fabdf947f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70427598 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70992629 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "tfidf = tfidf_matrix.toarray()\n",
    "print(tfidf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c565eb0d-6c5d-4abc-9976-b30059a4de52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5928992,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(positional_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aca95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
