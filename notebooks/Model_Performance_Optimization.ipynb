{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017b920d-4d1d-44f1-96df-7d43f0be90df",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0e4f219-a038-4eb9-af6f-a59697903096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spmatrix\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Ignore all runtime warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139f025-a986-42ae-b86f-8bf3ae073751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Feature Engineering classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a25b5d-7a31-45d9-b48f-ae8a51ba9699",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classification Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5f536b-f995-4f56-a7c3-5ea7262957d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/it042307/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class ClassFeatureEngineering:\n",
    "\n",
    "    \"\"\"\n",
    "    A class for implementing various text feature engineering techniques\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents\n",
    "        labels       { list }  : List of class labels corresponding to each text document\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, labels: list) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ClassFeatureEngineering with texts and labels\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts   : List of preprocessed text documents\n",
    "            labels  : List of class labels corresponding to each text document\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts or labels are empty or of different lengths\n",
    "        \"\"\"\n",
    "        if not texts or not labels:\n",
    "            raise ValueError(\"Input texts and labels cannot be empty\")\n",
    "        if len(texts) != len(labels):\n",
    "            raise ValueError(\"The number of texts and labels must be the same\")\n",
    "        \n",
    "        self.texts  = texts\n",
    "        self.labels = labels\n",
    "        self.classes = np.unique(labels)\n",
    "\n",
    "\n",
    "    def class_specific_vocabulary(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate a vocabulary specific to each class label and return a vectorizer and sparse matrix.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple : (vectorizer, sparse_matrix) \n",
    "                - vectorizer : A fitted CountVectorizer instance.\n",
    "                - sparse_matrix : A sparse matrix with the term frequencies for each class.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating class-specific vocabulary...\")\n",
    "            class_vocabs = defaultdict(set)\n",
    "            \n",
    "            # Collect class-specific vocabularies\n",
    "            for text, label in zip(self.texts, self.labels):\n",
    "                tokens = word_tokenize(text.lower())  \n",
    "                class_vocabs[label].update(tokens)\n",
    "            \n",
    "            print(\"Class-specific vocabulary created.\")\n",
    "            \n",
    "            # Create a unified list of all class-specific vocabularies\n",
    "            all_tokens = [' '.join(list(vocab)) for vocab in class_vocabs.values()]\n",
    "            \n",
    "            # Initialize and fit the CountVectorizer\n",
    "            vectorizer = CountVectorizer()\n",
    "            sparse_matrix = vectorizer.fit_transform(all_tokens)\n",
    "            \n",
    "            return vectorizer, sparse_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def label_aware_embeddings(self, embedding_dim=100) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate label-aware embeddings and return vectorizer and sparse matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "            embedding_dim : int\n",
    "                Dimensionality of the embedding vectors.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple : (vectorizer, sparse_matrix)\n",
    "                - vectorizer : Fitted CountVectorizer for label text.\n",
    "                - sparse_matrix : Sparse matrix representation of label embeddings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Generating label-aware embeddings...\")\n",
    "            \n",
    "            # Tokenize the texts\n",
    "            tokenized_texts = [word_tokenize(text.lower()) for text in self.texts]\n",
    "            \n",
    "            # Train Word2Vec embeddings\n",
    "            w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "            \n",
    "            # Prepare label-specific texts\n",
    "            label_texts = {label: ' '.join([word for text, lbl in zip(self.texts, self.labels) if lbl == label for word in word_tokenize(text.lower())]) for label in set(self.labels)}\n",
    "            \n",
    "            # Use CountVectorizer to create sparse matrix\n",
    "            vectorizer = CountVectorizer()\n",
    "            sparse_matrix = vectorizer.fit_transform(label_texts.values())\n",
    "            \n",
    "            print(\"Label-aware embeddings generated.\")\n",
    "            return vectorizer, sparse_matrix\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def hierarchical_class_features(self, hierarchy=None) -> tuple:\n",
    "    \n",
    "    Generate hierarchical class features and return vectorizer and sparse matrix.\n",
    "        \n",
    "    Parameters:\n",
    "        -----------\n",
    "            hierarchy : dict\n",
    "                A dictionary where keys are parent labels and values are lists of child labels.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple : (vectorizer, sparse_matrix)\n",
    "                - vectorizer : Fitted CountVectorizer for hierarchical features.\n",
    "                - sparse_matrix : Sparse matrix representation of hierarchical features.\n",
    "    \n",
    "        try:\n",
    "            print(\"Generating hierarchical class features...\")\n",
    "            \n",
    "            # Flatten hierarchy into parent-child paths\n",
    "            parent_child_pairs = []\n",
    "            for parent, children in hierarchy.items():\n",
    "                for child in children:\n",
    "                    parent_child_pairs.append((parent, child))\n",
    "            \n",
    "            # Create a mapping of labels to their hierarchical paths\n",
    "            label_to_hierarchy = defaultdict(list)\n",
    "            for parent, child in parent_child_pairs:\n",
    "                label_to_hierarchy[child].append(parent)\n",
    "                label_to_hierarchy[child].extend(label_to_hierarchy[parent])  # Add parent's hierarchy recursively\n",
    "            \n",
    "            # Flatten paths into text representations\n",
    "            hierarchical_texts = {label: ' '.join(path) for label, path in label_to_hierarchy.items()}\n",
    "            \n",
    "            # Use CountVectorizer to create sparse matrix\n",
    "            vectorizer = CountVectorizer()\n",
    "            sparse_matrix = vectorizer.fit_transform(hierarchical_texts.values())\n",
    "            \n",
    "            print(\"Hierarchical class features generated.\")\n",
    "            return vectorizer, sparse_matrix\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    \"\"\"\n",
    "    def multi_label_features(self) -> tuple:\n",
    "        \n",
    "        \"\"\"\n",
    "            Generate multi-label features and return vectorizer and sparse matrix.\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "                tuple : (vectorizer, sparse_matrix)\n",
    "                    - vectorizer : Fitted CountVectorizer for multi-label features.\n",
    "                    - sparse_matrix : Sparse matrix representation of multi-label features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Generating multi-label features...\")\n",
    "                \n",
    "            # Convert labels to sets if not already\n",
    "            multi_labels = [set(lbl) if isinstance(lbl, list) else {lbl} for lbl in self.labels]\n",
    "                \n",
    "            # Create multi-label strings for each sample\n",
    "            label_texts = [' '.join(label) for label in multi_labels]\n",
    "                \n",
    "            # Use CountVectorizer to create sparse matrix\n",
    "            vectorizer = CountVectorizer()\n",
    "            sparse_matrix = vectorizer.fit_transform(label_texts)\n",
    "                \n",
    "            print(\"Multi-label features generated.\")\n",
    "            return vectorizer, sparse_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af604e34-e506-415f-9cb8-27c98da626d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Contextual_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d38bc8-0202-4fac-803c-efe8f8c22f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE BY AVANTIKA ROY\n",
    "\n",
    "# Dependencies\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore all runtime warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Contextual_Features:\n",
    "    \"\"\"\n",
    "    A class for implementing various text feature engineering techniques\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create\n",
    "        \n",
    "        ngram_range  { tuple } : Range of n-grams to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None, ngram_range: tuple = (1, 3)) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureEngineering with texts and parameters\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit)\n",
    "            \n",
    "            ngram_range  : Range of n-grams to consider (min_n, max_n)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range  = ngram_range\n",
    "        \n",
    "    def window_based(self):\n",
    "        \"\"\"\n",
    "        Create Window Based Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts             : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Window-Based Contextual Features:...\")\n",
    "            vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            ngrams_features     = vectorizer.fit_transform(self.texts)\n",
    "            \n",
    "            return vectorizer, ngrams_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    '''def position_based(self):\n",
    "        \"\"\"\n",
    "        Create Position Based Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts              : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Position-Based Contextual Features:...\")\n",
    "            position_features = []\n",
    "            \n",
    "            position_vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                                  ngram_range  = self.ngram_range)\n",
    "\n",
    "            for doc in self.texts:\n",
    "                words = doc.split() \n",
    "            \n",
    "                position_features.extend([{\"word\": word, \"position\": idx} for idx, word in enumerate(words)])\n",
    "\n",
    "            return position_vectorizer, position_features\n",
    "\n",
    "        except Exception as e:\n",
    "            raise'''\n",
    "    \n",
    "    def position_based(self):\n",
    "        \"\"\"\n",
    "        Create Position Based Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts              : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Position-Based Contextual Features:...\")\n",
    "            position_vectorizer = CountVectorizer(max_features=self.max_features,\n",
    "                                                ngram_range=self.ngram_range)\n",
    "            \n",
    "            # First, fit and transform the texts normally\n",
    "            position_features = position_vectorizer.fit_transform(self.texts)\n",
    "            \n",
    "            return position_vectorizer, position_features\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    '''def generate_ngrams(self, n=3):\n",
    "        \"\"\"\n",
    "        Generate N-Grams\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        words         : List of words taken individually from the preprocessed text documents\n",
    "        n             : Individual words from the list\n",
    "        \"\"\"\n",
    "        print(\"Generating N-Grams:...\")\n",
    "        ngrams = []\n",
    "        \n",
    "        ngrams_vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                            ngram_range  = self.ngram_range)\n",
    "\n",
    "        for doc in self.texts:\n",
    "            words = doc.split() \n",
    "            ngrams.extend([tuple(words[i:i+n]) for i in range(len(words)-n+1)]) \n",
    "\n",
    "        return ngrams_vectorizer, ngrams'''\n",
    "    \n",
    "    def generate_ngrams(self, n=3):\n",
    "        \"\"\"\n",
    "        Generate N-Grams\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        words         : List of words taken individually from the preprocessed text documents\n",
    "        n             : Individual words from the list\n",
    "        \"\"\"\n",
    "        print(\"Generating N-Grams:...\")\n",
    "        \n",
    "        ngrams_vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                            ngram_range  = self.ngram_range)\n",
    "\n",
    "        ngrams_features = ngrams_vectorizer.fit_transform(self.texts)\n",
    "\n",
    "        return ngrams_vectorizer, ngrams_features\n",
    "\n",
    "\n",
    "    def cross_document(self):\n",
    "        \"\"\"\n",
    "        Create Cross Document Feature Engineering with texts and parameters\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        texts             : List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        try: \n",
    "            print(\"Creating Cross Document Contextual Feature Engineering:...\")\n",
    "            vectorizer   = TfidfVectorizer(max_features = self.max_features, \n",
    "                                           ngram_range  = self.ngram_range)\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.texts)\n",
    "            return vectorizer, tfidf_matrix\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0c20c-2acf-4d88-92b2-bd7a82cad8f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126b6bf3-4675-487f-99f5-421500575ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "class Semantic_Feature_Engineering:\n",
    "    \n",
    "    \"\"\"\n",
    "    A class for implementing various semantic feature engineering techniques.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents.\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize Semantic_Feature_Engineering with texts and parameters.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents.\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit).\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def word2vec_cbow(self, vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4) -> tuple:\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate semantic features using Word2Vec (CBOW) and return the feature matrix and vectorizer.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            vector_size : Dimensionality of word embeddings (default: 100).\n",
    "            \n",
    "            window      : Context window size (default: 5).\n",
    "            \n",
    "            min_count   : Ignores words with frequency lower than this (default: 1).\n",
    "            \n",
    "            workers     : Number of worker threads to train the model (default: 4).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - np.ndarray : Document-level feature matrix (each document represented as the average of its word vectors).\n",
    "                - Word2Vec   : The trained Word2Vec model (vectorizer).\n",
    "        \"\"\"\n",
    "    \n",
    "        tokenized_texts          = [doc.split() for doc in self.texts]\n",
    "        \n",
    "        w2v_model                = Word2Vec(sentences   = tokenized_texts, \n",
    "                                            vector_size = vector_size, \n",
    "                                            window      = window, \n",
    "                                            min_count   = min_count, \n",
    "                                            workers     = workers,\n",
    "                                            sg          = 0\n",
    "                                            )\n",
    "        \n",
    "        features                 = []\n",
    "        \n",
    "        for tokens in tokenized_texts:\n",
    "            \n",
    "            vectors              = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "            \n",
    "            if vectors:\n",
    "                document_vector  = np.mean(vectors, axis=0)\n",
    "            \n",
    "            else:\n",
    "                document_vector  = np.zeros(vector_size)\n",
    "            \n",
    "            features.append(document_vector)\n",
    "        \n",
    "        feature_matrix           = np.array(features)\n",
    "        \n",
    "        if self.max_features is not None and self.max_features < vector_size:\n",
    "            feature_matrix       = feature_matrix[:, :self.max_features]\n",
    "        \n",
    "        return feature_matrix, w2v_model\n",
    "    \n",
    "    def glove(self, glove_path: str, embedding_dim: int = 100) -> tuple:\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate semantic features using GloVe and return the feature matrix and embedding dictionary.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            glove_path        : Path to the GloVe embeddings file.\n",
    "            \n",
    "            embedding_dim     : Dimensionality of GloVe embeddings (default: 100).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple:\n",
    "                - np.ndarray  : Document-level feature matrix (each document represented as the average of its word vectors).\n",
    "                - dict        : The GloVe embedding dictionary.\n",
    "        \"\"\"\n",
    "    \n",
    "        glove_embeddings                = {}\n",
    "        \n",
    "        with open(glove_path, 'r', encoding = 'utf-8') as f:\n",
    "            \n",
    "            for line in f:\n",
    "                values                  = line.split()\n",
    "                word                    = values[0]\n",
    "                vector                  = np.asarray(values[1:], dtype='float32')\n",
    "                glove_embeddings[word]  = vector\n",
    "        \n",
    "        tokenized_texts                 = [doc.split() for doc in self.texts]\n",
    "        \n",
    "        features                        = []\n",
    "        \n",
    "        for tokens in tokenized_texts:\n",
    "    \n",
    "            vectors                     = [glove_embeddings[word] for word in tokens if word in glove_embeddings]\n",
    "            \n",
    "            if vectors:\n",
    "                document_vector         = np.mean(vectors, axis=0)\n",
    "            \n",
    "            else:\n",
    "                document_vector         = np.zeros(embedding_dim)\n",
    "            \n",
    "            features.append(document_vector)\n",
    "        \n",
    "        feature_matrix                  = np.array(features)\n",
    "        \n",
    "        if self.max_features is not None and self.max_features < embedding_dim:\n",
    "            feature_matrix              = feature_matrix[:, :self.max_features]\n",
    "        \n",
    "        return feature_matrix, glove_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d789726-66fd-47e0-b062-5c72456a0bd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe80903-542d-4778-985e-9e6fe51f4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Statistical_Feature_Engineering():\n",
    "    \"\"\"\n",
    "    A class for statistical feature engineering.\n",
    "    Attributes:\n",
    "    ----------\n",
    "    vectorizer : CountVectorizer instance for text vectorization.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, max_features=1000):\n",
    "        \"\"\"\n",
    "        Intializes the Statistical_Feature_Engineering.\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = CountVectorizer(max_features=self.max_features)\n",
    "\n",
    "    def document_statistics(self,df):\n",
    "        \"\"\"\n",
    "        Calculates basic documnet statistics i.e. Character Count, Word Count, Sentence Count, Average Word Length(AWL), \n",
    "        Average Sentence Length(ASL), Unique Word Ratio(UWR).\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        df {DataFrame} : Output data with the calculated document statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df['char_count'] = df['cleaned_review'].apply(lambda x: len(x))\n",
    "        df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "        df['sent_count'] = df['review'].apply(lambda x: len(sent_tokenize(x)))\n",
    "        df['AWL'] = df['char_count'].div(df['word_count'])\n",
    "        df['ASL'] = df['word_count'].div(df['sent_count'])\n",
    "        df['unique_word_count'] = df['cleaned_review'].apply(lambda x: len(set(word_tokenize(x))))\n",
    "        df['UWR'] = df['unique_word_count'].div(df['word_count'])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def readability_score(self, df, score='FRE'):\n",
    "        \"\"\"\n",
    "        Calculates the readability scores i.e. Flesch Readine Ease(FRE), Gunning Fog Index(GFI), SMOG Index(SMOG.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        score {str} : score type {'FRE', 'GFI', SMOG}.\n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        fre {series} : FRE scores.\n",
    "\n",
    "        gfi {series} : GFI scores.\n",
    "\n",
    "        smog {series} : SMOG scores.\n",
    "        \n",
    "        \"\"\"\n",
    "        if(score == 'FRE'):\n",
    "            fre = df['cleaned_review'].apply(textstat.flesch_reading_ease)\n",
    "            return fre\n",
    "        elif(score == 'GFI'):\n",
    "            gfi = df['cleaned_review'].apply(textstat.gunning_fog)\n",
    "            return gfi\n",
    "        elif(score == 'SMOG'):\n",
    "            smog = df['cleaned_review'].apply(textstat.smog_index)\n",
    "            return smog\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported score type. Choose from 'FRE', 'GFI', or 'SMOG'.\")\n",
    "        \n",
    "\n",
    "    def frequency_distribution(self, df,column, fit_transform=False):\n",
    "        \"\"\"\n",
    "        Calculates the word counts in each document.\n",
    "\n",
    "        df {DataFrame} : Input Data.\n",
    "\n",
    "        column {str} : Column name for calculating the frequency distribution.\n",
    "        \"\"\"\n",
    "        if fit_transform:\n",
    "            X = self.vectorizer.fit_transform(df[column])\n",
    "        else:\n",
    "            X = self.vectorizer.transform(df[column])\n",
    "        bow = pd.DataFrame(X.toarray(), columns=self.vectorizer.get_feature_names_out())\n",
    "        return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab66a82-2774-45cc-b82f-3fa8efae06f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570f27d0-df54-414c-bbdc-d2d02f4ed6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "class TextFeatureEngineering:\n",
    "    \"\"\"\n",
    "    A class for implementing various text feature engineering techniques\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        texts        { list }  : List of preprocessed text documents\n",
    "        \n",
    "        max_features  { int }  : Maximum number of features to create\n",
    "        \n",
    "        ngram_range  { tuple } : Range of n-grams to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, max_features: int = None, ngram_range: tuple = (1, 3)) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureEngineering with texts and parameters\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            texts        : List of preprocessed text documents\n",
    "            \n",
    "            max_features : Maximum number of features (None for no limit)\n",
    "            \n",
    "            ngram_range  : Range of n-grams to consider (min_n, max_n)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError   : If texts is empty or parameters are invalid\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            raise ValueError(\"Input texts cannot be empty\")\n",
    "            \n",
    "        self.texts        = texts\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range  = ngram_range\n",
    "        \n",
    "        \n",
    "    def create_binary_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create binary bag-of-words features (presence/absence)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer\n",
    "                                          - Binary document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating binary bag-of-words features...\")\n",
    "            vectorizer = CountVectorizer(binary       = True,\n",
    "                                         max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} binary features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_count_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create count-based bag-of-words features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer\n",
    "                                          - Count document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating count-based bag-of-words features...\")\n",
    "            vectorizer = CountVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} count-based features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_frequency_bow(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create frequency-based bag-of-words features (term frequency)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - Term frequency document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating frequency-based bag-of-words features...\")\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(use_idf      = False,\n",
    "                                         max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} frequency-based features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_tfidf(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create TF-IDF features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - TF-IDF document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating TF-IDF features...\")\n",
    "            vectorizer = TfidfVectorizer(max_features = self.max_features,\n",
    "                                         ngram_range  = self.ngram_range)\n",
    "            \n",
    "            features   = vectorizer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} TF-IDF features\")\n",
    "            \n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_standardized_tfidf(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create Standardized TF-IDF features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted TfidfVectorizer\n",
    "                                          - Standardized TF-IDF document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating Standardized TF-IDF features...\")\n",
    "            vectorizer          = TfidfVectorizer(max_features = self.max_features, \n",
    "                                                  ngram_range  = self.ngram_range)\n",
    "            \n",
    "            tfidf_matrix        = vectorizer.fit_transform(self.texts)\n",
    "            \n",
    "            scaler              = StandardScaler(with_mean = False)\n",
    "            \n",
    "            standardized_matrix = scaler.fit_transform(tfidf_matrix)\n",
    "            \n",
    "            print(f\"Created {standardized_matrix.shape[1]} standardized TF-IDF features\")\n",
    "            return vectorizer, standardized_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def _create_bm25_variant(self, variant: str, k1: float = 1.5, b: float = 0.75, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Unified private method to create BM25 variant features.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            variant      : Specify the BM25 variant (\"BM25\", \"BM25F\", \"BM25L\", \"BM25+\", \"BM25T\")\n",
    "            k1           : Term frequency saturation parameter (default: 1.5)\n",
    "            b            : Length normalization parameter (default: 0.75)\n",
    "            delta        : Free parameter for certain variants (default: 1.0)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple }    : Tuple containing:\n",
    "                           - Custom transformer for the specified BM25 variant\n",
    "                           - BM25 variant document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Creating {variant} features...\")\n",
    "\n",
    "            class BM25VariantTransformer(BaseEstimator, TransformerMixin):\n",
    "                def __init__(self, k1=1.5, b=0.75, delta=1.0, variant=\"BM25\", max_features=None):\n",
    "                    self.k1               = k1\n",
    "                    self.b                = b\n",
    "                    self.delta            = delta\n",
    "                    self.variant          = variant\n",
    "                    self.max_features     = max_features\n",
    "                    self.count_vectorizer = CountVectorizer(max_features = self.max_features)\n",
    "\n",
    "                def fit(self, texts):\n",
    "                    # Calculate IDF and average document length\n",
    "                    X                   = self.count_vectorizer.fit_transform(texts)\n",
    "                    self.avg_doc_length = X.sum(axis=1).mean()\n",
    "                    n_docs              = len(texts)\n",
    "                    df                  = np.bincount(X.indices, minlength=X.shape[1])\n",
    "                    self.idf            = np.log((n_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "                    return self\n",
    "\n",
    "                def transform(self, texts):\n",
    "                    X           = self.count_vectorizer.transform(texts)\n",
    "                    doc_lengths = X.sum(axis=1).A1\n",
    "                    rows, cols  = X.nonzero()\n",
    "                    data        = list()\n",
    "\n",
    "                    for i, j in zip(rows, cols):\n",
    "                        tf = X[i, j]\n",
    "\n",
    "                        if (self.variant == \"BM25\"):\n",
    "                            numerator   = tf * (self.k1 + 1)\n",
    "                            denominator = tf + self.k1 * (1 - self.b + self.b * doc_lengths[i] / self.avg_doc_length)\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif (self.variant == \"BM25F\"):\n",
    "                            score = self.idf[j] * (tf / (self.k1 + tf))\n",
    "\n",
    "                        elif (self.variant == \"BM25L\"):\n",
    "                            numerator   = tf + self.delta\n",
    "                            denominator = tf + self.delta + self.k1 * (1 - self.b + self.b * doc_lengths[i] / self.avg_doc_length)\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif (self.variant == \"BM25+\"):\n",
    "                            numerator   = tf + self.delta\n",
    "                            denominator = tf + self.k1\n",
    "                            score       = self.idf[j] * numerator / denominator\n",
    "                        \n",
    "                        elif (self.variant == \"BM25T\"):\n",
    "                            score = self.idf[j] * (tf * np.log(1 + tf))\n",
    "                        \n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown variant: {self.variant}\")\n",
    "                        \n",
    "                        data.append(score)\n",
    "\n",
    "                    return csr_matrix((data, (rows, cols)), shape=X.shape)\n",
    "\n",
    "                def get_feature_names_out(self):\n",
    "                    \"\"\"\n",
    "                    Return the feature names from the underlying CountVectorizer.\n",
    "                    \"\"\"\n",
    "                    return self.count_vectorizer.get_feature_names_out()\n",
    "\n",
    "            transformer = BM25VariantTransformer(k1           = k1, \n",
    "                                                 b            = b, \n",
    "                                                 delta        = delta,\n",
    "                                                 variant      = variant, \n",
    "                                                 max_features = self.max_features)\n",
    "\n",
    "            features    = transformer.fit_transform(self.texts)\n",
    "            print(f\"Created {features.shape[1]} {variant} features\")\n",
    "            return transformer, features\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def create_bm25(self, k1: float = 1.5, b: float = 0.75) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25 features\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant = \"BM25\", \n",
    "                                         k1      = k1, \n",
    "                                         b       = b)\n",
    "\n",
    "\n",
    "    def create_bm25f(self, k1: float = 1.5) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25F features\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant = \"BM25F\", \n",
    "                                         k1      = k1)\n",
    "\n",
    "\n",
    "    def create_bm25l(self, k1: float = 1.5, b: float = 0.75, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25L features\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant = \"BM25L\", \n",
    "                                         k1      = k1, \n",
    "                                         b       = b, \n",
    "                                         delta   = delta)\n",
    "\n",
    "\n",
    "    def create_bm25_plus(self, k1: float = 1.5, delta: float = 1.0) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25+ features\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant = \"BM25+\", \n",
    "                                         k1      = k1, \n",
    "                                         delta   = delta)\n",
    "\n",
    "\n",
    "    def create_bm25t(self, k1: float = 1.5) -> tuple:\n",
    "        \"\"\"\n",
    "        Create BM25T features\n",
    "        \"\"\"\n",
    "        return self._create_bm25_variant(variant = \"BM25T\", \n",
    "                                         k1      = k1)\n",
    "\n",
    "\n",
    "    def create_skipgrams(self, k: int = 2) -> tuple:\n",
    "        \"\"\"\n",
    "        Create skipgram features\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            k { int } : Skip distance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer for skipgrams\n",
    "                                          - Skipgram document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating skipgram features...\")\n",
    "            \n",
    "            def generate_skipgrams(text: str) -> str:\n",
    "                words     = text.split()\n",
    "                skipgrams = list()\n",
    "                \n",
    "                for i in range(len(words) - k - 1):\n",
    "                    skipgram = f\"{words[i]}_{words[i + k + 1]}\"\n",
    "                    skipgrams.append(skipgram)\n",
    "                    \n",
    "                return ' '.join(skipgrams)\n",
    "            \n",
    "            processed_texts = [generate_skipgrams(text) for text in self.texts]\n",
    "            \n",
    "            vectorizer      = CountVectorizer(max_features=self.max_features)\n",
    "            features        = vectorizer.fit_transform(processed_texts)\n",
    "            \n",
    "            print(f\"Created {features.shape[1]} skipgram features\")\n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_positional_ngrams(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Create positional n-gram features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Fitted CountVectorizer for positional n-grams\n",
    "                                          - Positional n-gram document-term matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating positional n-gram features...\")\n",
    "            \n",
    "            def generate_positional_ngrams(text: str) -> str:\n",
    "                words      = text.split()\n",
    "                pos_ngrams = list()\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    for n in range(self.ngram_range[0], min(self.ngram_range[1] + 1, len(words) - i + 1)):\n",
    "                        ngram     = '_'.join(words[i:i+n])\n",
    "                        pos_ngram = f\"pos{i}_{ngram}\"\n",
    "                        pos_ngrams.append(pos_ngram)\n",
    "                        \n",
    "                return ' '.join(pos_ngrams)\n",
    "            \n",
    "            processed_texts = [generate_positional_ngrams(text) for text in self.texts]\n",
    "            \n",
    "            vectorizer      = CountVectorizer(max_features = self.max_features)\n",
    "            \n",
    "            features        = vectorizer.fit_transform(processed_texts)\n",
    "            \n",
    "            print(f\"Created {features.shape[1]} positional n-gram features\")\n",
    "            return vectorizer, features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    def create_all_features(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create all available feature types\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { dict } : Dictionary mapping feature names to their vectorizer and feature matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Creating all feature types...\")\n",
    "            features                      = dict()\n",
    "            \n",
    "            # Create all feature types\n",
    "            features['binary_bow']        = self.create_binary_bow()\n",
    "            features['count_bow']         = self.create_count_bow()\n",
    "            features['frequency_bow']     = self.create_frequency_bow()\n",
    "            features['tfidf']             = self.create_tfidf()\n",
    "            features['bm25']              = self.create_bm25()\n",
    "            features['skipgrams']         = self.create_skipgrams()\n",
    "            features['positional_ngrams'] = self.create_positional_ngrams()\n",
    "            \n",
    "            print(\"Created all feature types successfully\")\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91a21c-639a-474a-bbb8-c7b196277903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bd405a-39e4-4dbb-b151-2d8a69773f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_ratings_df = pd.read_csv(filepath_or_buffer = '../data/IMDB_Dataset.csv',\n",
    "                              index_col          = None)\n",
    "\n",
    "imdb_ratings_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067b4f8-cee6-493b-92aa-f1d13f61f45a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Creating test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83cff09-b2f3-4b09-9362-7893019434c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\"This movie is speedy enough with plot twists, but hard to understand the connection between plots.\",\n",
    "             \"Seriously, this is the best movie I've ever watched! Everything was flawless!\",\n",
    "             \"The storyline was okay, but the acting was just not up to the mark.\",\n",
    "             \"A complete disaster of a movie. Don't waste your time.\",\n",
    "             \"I can't believe how amazing this was. Totally worth it!\",\n",
    "             \"The movie had its moments, but overall, it felt like something was missing.\",\n",
    "             \"I absolutely loved the cinematography, but the acting was subpar.\",\n",
    "             \"The film is an excellent example of how not to make a movie.\",\n",
    "             \"It's hard to imagine how anyone could dislike this masterpiece!\",\n",
    "             \"The trailer was better than the actual movie. Felt cheated.\",\n",
    "             \"A rollercoaster of emotions! Highly recommend watching this.\",\n",
    "             \"An average movie with nothing new to offer.\",\n",
    "             \"The pacing was terrible, and the climax was predictable.\",\n",
    "             \"Wow, just wow. This is how a movie should be made!\",\n",
    "             \"A decent watch for a lazy weekend. Not groundbreaking, but enjoyable.\",\n",
    "             \"The director has outdone themselves; what a phenomenal movie!\",\n",
    "             \"More hype than substance. A complete letdown.\",\n",
    "             \"Good visuals, decent music, but lacked a solid script.\",\n",
    "             \"A masterpiece in every sense. This will stay with me forever.\",\n",
    "             \"Mediocre at best. Not worth the ticket price.\",\n",
    "             \"A fresh take on a tired genre. Highly recommend it!\",\n",
    "             \"Overrated and boring. Nothing special about it.\",\n",
    "             \"This is one of those movies you'll regret missing. A must-watch!\",\n",
    "             \"Predictable plot, but the performances were top-notch.\",\n",
    "             \"It's a bad movie if you're looking for entertainment.\",\n",
    "             \"Can't believe I sat through the entire thing. A waste of time.\",\n",
    "             \"Finally, a movie that gets it right. Loved every minute of it!\",\n",
    "             \"A forgettable movie with no real impact.\",\n",
    "             \"An extraordinary journey that left me speechless. Bravo!\",\n",
    "             \"The humor was forced, and the dialogue was cringeworthy.\",\n",
    "             \"A solid movie with a gripping narrative. Well done!\",\n",
    "             \"The music was fantastic, but the rest of the movie was average.\",\n",
    "             \"Ironic how they managed to make something so beautiful look so bland.\",\n",
    "             \"An epic conclusion to a fantastic series. Couldnt have been better!\",\n",
    "             \"The movie tries too hard to be funny and fails miserably.\",\n",
    "             \"A fresh and engaging story with relatable characters.\",\n",
    "             \"All style, no substance. Disappointing.\",\n",
    "             \"A breath of fresh air! One of the best movies this year.\",\n",
    "             \"The plot was all over the place, but it was fun to watch.\",\n",
    "             \"Couldn't make it through the first half. Painful to sit through.\",\n",
    "             \"An unexpectedly beautiful film that touched my heart.\",\n",
    "             \"Trying to understand why this movie exists is more entertaining than the movie itself.\",\n",
    "             \"Every second of this movie was a blessing. Pure cinematic joy.\",\n",
    "             \"The lead actor was the only saving grace in an otherwise dull film.\",\n",
    "             \"A pretentious attempt at storytelling that falls flat.\",\n",
    "             \"I didnt expect much, but this movie surprised me in the best way.\",\n",
    "             \"A series of poorly executed clichs masquerading as a story.\",\n",
    "             \"This is not just a movie; its an experience. Brilliant!\",\n",
    "             \"A slog of a movie with a laughably bad ending.\",\n",
    "            ]\n",
    "\n",
    "\n",
    "sentiments = [\"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"positive\", \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",    \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",    \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"negative\", \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"positive\",  \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",  \n",
    "              \"positive\",    \n",
    "              \"negative\",   \n",
    "              \"positive\",   \n",
    "              \"negative\",  \n",
    "              \"positive\",   \n",
    "              \"negative\",\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b644c7a-fb72-43c4-8613-182556364474",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {'review' : test_data,\n",
    "                  'sentiment' : sentiments}\n",
    "test_df        = pd.DataFrame(test_data_dict)\n",
    "# test_df.to_csv(path_or_buf='../data/test_data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897b5e7f-765b-4b5a-850c-d5f3a4df4f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is speedy enough with plot twists, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seriously, this is the best movie I've ever wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The storyline was okay, but the acting was jus...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A complete disaster of a movie. Don't waste yo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can't believe how amazing this was. Totally ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  This movie is speedy enough with plot twists, ...  negative\n",
       "1  Seriously, this is the best movie I've ever wa...  positive\n",
       "2  The storyline was okay, but the acting was jus...  positive\n",
       "3  A complete disaster of a movie. Don't waste yo...  negative\n",
       "4  I can't believe how amazing this was. Totally ...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fe4cd-bdbf-4427-8276-916fc849807f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e593b724-24bd-412c-a8b5-867325b8839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureSelector:\n",
    "    \"\"\"\n",
    "    A class for implementing various feature selection techniques for text data\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        X           { spmatrix } : Feature matrix\n",
    "        \n",
    "        y           { ndarray }  : Target labels\n",
    "\n",
    "        feature_names { list }   : Names of features\n",
    "        \n",
    "        n_features    { int }    : Number of features to select\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: spmatrix, y: np.ndarray, feature_names: list, n_features: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize TextFeatureSelector with feature matrix and labels\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            X             : Sparse feature matrix\n",
    "            \n",
    "            y             : Target labels\n",
    "            \n",
    "            feature_names : List of feature names\n",
    "            \n",
    "            n_features    : Number of features to select (default: 10% of features)\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError    : If inputs are invalid or incompatible\n",
    "        \"\"\"\n",
    "        if (X.shape[0] != len(y)):\n",
    "            raise ValueError(\"Number of samples in X and y must match\")\n",
    "            \n",
    "        if (X.shape[1] != len(feature_names)):\n",
    "            raise ValueError(\"Number of features must match length of feature_names\")\n",
    "            \n",
    "        self.X             = X\n",
    "        self.y             = y\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features    = n_features or int(0.1 * X.shape[1])  # Default to 10% of features\n",
    "        \n",
    "        \n",
    "    def chi_square_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform chi-square feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Chi-square scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing chi-square feature selection...\")\n",
    "            \n",
    "            # Scale features to non-negative for chi-square\n",
    "            scaler            = MinMaxScaler()\n",
    "            X_scaled          = scaler.fit_transform(self.X.toarray())\n",
    "            \n",
    "            # Apply chi-square selection\n",
    "            selector          = SelectKBest(score_func = chi2, \n",
    "                                            k          = self.n_features)\n",
    "            \n",
    "            selector.fit(X_scaled, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using chi-square\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def information_gain_selection(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform information gain feature selection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple } : Tuple containing: - Selected feature indices\n",
    "                                          - Information gain scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing information gain selection...\")\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            selector          = SelectKBest(score_func = mutual_info_classif, \n",
    "                                            k          = self.n_features)\n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and scores\n",
    "            selected_features = np.where(selector.get_support())[0]\n",
    "            scores            = selector.scores_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_idx        = np.argsort(scores)[::-1]\n",
    "            selected_features = sorted_idx[:self.n_features]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using information gain\")\n",
    "            \n",
    "            return selected_features, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def correlation_based_selection(self, threshold: float = 0.8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform correlation-based feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            threshold { float } : Correlation threshold for feature removal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "               { ndarray }      :  Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing correlation-based selection...\")\n",
    "            \n",
    "            # Convert sparse matrix to dense for correlation calculation\n",
    "            X_dense         = self.X.toarray()\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            corr_matrix     = np.corrcoef(X_dense.T)\n",
    "            \n",
    "            # Find highly correlated feature pairs\n",
    "            high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "            \n",
    "            # Keep track of features to remove\n",
    "            to_remove       = set()\n",
    "            \n",
    "            # For each pair of highly correlated features\n",
    "            for i, j in zip(*high_corr_pairs):\n",
    "                if ((i != j) and (i not in to_remove) and (j not in to_remove)):\n",
    "                    # Calculate correlation with target for both features\n",
    "                    corr_i = mutual_info_score(X_dense[:, i], self.y)\n",
    "                    corr_j = mutual_info_score(X_dense[:, j], self.y)\n",
    "                    \n",
    "                    # Remove feature with lower correlation to target\n",
    "                    if (corr_i < corr_j):\n",
    "                        to_remove.add(i)\n",
    "                        \n",
    "                    else:\n",
    "                        to_remove.add(j)\n",
    "            \n",
    "            # Get selected features\n",
    "            all_features      = set(range(self.X.shape[1]))\n",
    "            selected_features = np.array(list(all_features - to_remove))\n",
    "            \n",
    "            # Select top k features if more than n_features remain\n",
    "            if (len(selected_features) > self.n_features):\n",
    "                # Calculate mutual information for remaining features\n",
    "                mi_scores         = mutual_info_classif(self.X[:, selected_features], self.y)\n",
    "                top_k_idx         = np.argsort(mi_scores)[::-1][:self.n_features]\n",
    "                selected_features = selected_features[top_k_idx]\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using correlation-based selection\")\n",
    "            \n",
    "            return selected_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def recursive_feature_elimination(self, estimator = None, cv: int = 5) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform Recursive Feature Elimination with cross-validation\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator  : Classifier to use (default: LogisticRegression)\n",
    "            cv         : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            { tuple }  : Tuple containing: - Selected feature indices\n",
    "                                           - Feature importance rankings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing recursive feature elimination...\")\n",
    "            \n",
    "            # Use logistic regression if no estimator provided\n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            # Perform RFE with cross-validation\n",
    "            selector = RFECV(estimator              = estimator,\n",
    "                             min_features_to_select = self.n_features,\n",
    "                             cv                     = cv,\n",
    "                             n_jobs                 = -1)\n",
    "            \n",
    "            selector.fit(self.X, self.y)\n",
    "            \n",
    "            # Get selected features and rankings\n",
    "            selected_features = np.where(selector.support_)[0]\n",
    "            rankings          = selector.ranking_\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features using RFE\")\n",
    "            \n",
    "            return selected_features, rankings\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "           \n",
    "        \n",
    "    def forward_selection(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward feature selection\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing forward selection...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            selected_features  = list()\n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            for i in tqdm(range(self.n_features)):\n",
    "                best_score   = -np.inf\n",
    "                best_feature = None\n",
    "                \n",
    "                # Try adding each remaining feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = selected_features + [feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score   = avg_score\n",
    "                        best_feature = feature\n",
    "                \n",
    "                if (best_feature is not None):\n",
    "                    selected_features.append(best_feature)\n",
    "                    remaining_features.remove(best_feature)\n",
    "                \n",
    "            print(f\"Selected {len(selected_features)} features using forward selection\")\n",
    "            \n",
    "            return np.array(selected_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            \n",
    "    def backward_elimination(self, estimator = None, cv: int = 5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward feature elimination\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            estimator : Classifier to use (default: LogisticRegression)\n",
    "            \n",
    "            cv        : Number of cross-validation folds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            Selected feature indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Performing backward elimination...\")\n",
    "            \n",
    "            if (estimator is None):\n",
    "                estimator = LogisticRegression(max_iter=1000)\n",
    "            \n",
    "            remaining_features = list(range(self.X.shape[1]))\n",
    "            \n",
    "            while len(remaining_features) > self.n_features:\n",
    "                best_score    = -np.inf\n",
    "                worst_feature = None\n",
    "                \n",
    "                # Try removing each feature\n",
    "                for feature in remaining_features:\n",
    "                    current_features = [f for f in remaining_features if f != feature]\n",
    "                    X_subset         = self.X[:, current_features]\n",
    "                    \n",
    "                    # Calculate cross-validation score\n",
    "                    scores = cross_val_score(estimator, \n",
    "                                             X_subset, \n",
    "                                             self.y,\n",
    "                                             cv      = cv, \n",
    "                                             scoring = 'accuracy')\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    if (avg_score > best_score):\n",
    "                        best_score    = avg_score\n",
    "                        worst_feature = feature\n",
    "                \n",
    "                if worst_feature is not None:\n",
    "                    remaining_features.remove(worst_feature)\n",
    "            \n",
    "            print(f\"Selected {len(remaining_features)} features using backward elimination\")\n",
    "            return np.array(remaining_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921915d-bbcd-4755-858e-80bb143ad208",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc33ef38-5861-45a5-afb2-fbb28d1d881d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data through cleaning, tokenization, and normalization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "        lemmatizer : WordNetLemmatizer instance for word lemmatization\n",
    "        \n",
    "        stop_words : Set of stopwords to be removed from text\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TextPreprocessor with required NLTK resources\n",
    "        \n",
    "        Raises:\n",
    "        -------\n",
    "            LookupError : If required NLTK resources cannot be downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Download required NLTK data\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            \n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        except LookupError as e:\n",
    "            raise\n",
    "    \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text by removing HTML tags, special characters,\n",
    "        and applying text normalization techniques\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            text { str }      : Input text to be cleaned\n",
    "            \n",
    "        Raises:\n",
    "        -------\n",
    "            ValueError        : If input text is None or empty\n",
    "            \n",
    "            TextCleaningError : If any error occurs at any step of text cleaning process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "                { str }       : Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if ((not text) or (not isinstance(text, str))):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "            \n",
    "        try:\n",
    "            # Remove HTML tags\n",
    "            text   = re.sub('<[^>]*>', '', text)\n",
    "            \n",
    "            # Remove special characters and digits\n",
    "            text   = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Convert to lowercase\n",
    "            text   = text.lower()\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        except Exception as TextCleaningError:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cde429-3d6a-45ba-9a2a-844300d7be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [00:53<00:00, 928.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor                  = TextPreprocessor()\n",
    "tqdm.pandas()\n",
    "# Add a new column to the original DataFrame to store the cleaned texts\n",
    "imdb_ratings_df['clean_text'] = imdb_ratings_df['review'].progress_apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e21732-ad65-4ea5-9f77-47a5b3182a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                          clean_text  \n",
       "0  one reviewer mentioned watching oz episode you...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80dfac92-02f2-413f-a631-8ebd8ac53739",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_level = TextFeatureEngineering(list(imdb_ratings_df['clean_text']), max_features=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf824ff0-af5c-4be6-924c-a9717be07547",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b74f17f-5418-40cc-b7ec-56798d6d45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic'          : LogisticRegression(),\n",
    "    'SVM_poly'          : SVC(kernel='poly'),\n",
    "    'SVM_rbf'           : SVC(kernel='rbf'),\n",
    "    'SVM_sig'           : SVC(kernel='sigmoid'),\n",
    "    'Random_Forest'     : RandomForestClassifier(),\n",
    "    'Multi_NaiveBayes'  : MultinomialNB(),\n",
    "    'Gauss_NaiveBayes'  : GaussianNB(),\n",
    "    'Gradient_Boost'    : GradientBoostingClassifier(),\n",
    "    'AdaBoost'          : AdaBoostClassifier(),\n",
    "    'LightGBM'          : LGBMClassifier(),\n",
    "    'LogisticDT'        : DecisionTreeClassifier(),\n",
    "    'MultiLayerPercep'  : MLPClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9828d304-9664-40e2-a89b-846b2f4f0472",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ModelSelector:\n",
    "    \"\"\"\n",
    "    A class for selecting the best model for sentiment analysis task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, feature_eng, vectorizers, selected_feature_indices,test_size=0.2, random_state=42, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the ModelSelector by splitting the data.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            X                        : Feature matrix (sparse matrix or ndarray)\n",
    "            \n",
    "            y                        : Target labels (array-like)\n",
    "            \n",
    "            feature_eng              : Instance of TextFeatureEngineering\n",
    "            \n",
    "            vectorizers              : Tuple of vectorizers used for feature transformation\n",
    "            \n",
    "            selected_feature_indices : Indices of selected features after feature selection\n",
    "            \n",
    "            test_size                : Proportion of data to use for testing (default: 0.2)\n",
    "            \n",
    "            random_state             : Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size    = test_size, \n",
    "                                                                                random_state = random_state)\n",
    "        \n",
    "        self.feature_eng                                     = feature_eng\n",
    "        self.vectorizers                                     = vectorizers\n",
    "        self.selected_feature_indices                        = selected_feature_indices\n",
    "\n",
    "        \n",
    "    def train_model(self, model_type:str = \"logistic_regression\", kernel=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train a sentiment analysis model.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model_type { str } : Type of model to train (e.g: \"logistic_regression\", \"svm\", \"random_forest\")\n",
    "            \n",
    "            kernel     { str } : Kernel type for SVM (e.g., \"linear\", \"poly\", \"rbf\", \"sigmoid\")\n",
    "            \n",
    "            kwargs             : Additional arguments for the model initialization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        if (model_type == \"logistic_regression\"):\n",
    "            model = LogisticRegression(max_iter = 1000, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"svm\"):\n",
    "            \n",
    "            if (kernel is None):\n",
    "                # Default kernel\n",
    "                kernel = \"rbf\"  \n",
    "                \n",
    "            model = SVC(kernel = kernel, **kwargs)\n",
    "            \n",
    "        elif (model_type == \"random_forest\"):\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "            \n",
    "        elif model_type == \"naive_bayes\":\n",
    "            model = MultinomialNB(**kwargs)\n",
    "\n",
    "        elif model_type == \"lightgbm\":\n",
    "            model = LGBMClassifier(**kwargs)\n",
    "\n",
    "        elif model_type == \"logistic_model_tree\":\n",
    "            model = DecisionTreeClassifier(**kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose from: 'logistic_regression', 'svm', 'random_forest'\")\n",
    "\n",
    "        print(f\"Training {model_type}...\")\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model on the test set\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model : Trained model\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred   = model.predict(self.X_test)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report   = classification_report(self.y_test, y_pred)\n",
    "        cm       = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\"accuracy\"              : accuracy,\n",
    "                \"classification_report\" : report,\n",
    "                \"confusion_matrix\"      : cm,\n",
    "               }\n",
    "\n",
    "    \n",
    "    def test_on_unseen_data(self, model, unseen_texts):\n",
    "        \"\"\"\n",
    "        Test the model on unseen data\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model         : Trained model\n",
    "            \n",
    "            unseen_texts  : List of unseen text data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Predictions for the unseen data\n",
    "        \"\"\"\n",
    "        print(\"Processing unseen data...\")\n",
    "\n",
    "        # Preprocess unseen data (implement preprocessing in the feature engineering class)\n",
    "        binary_features          = self.vectorizers[0].transform(unseen_texts)\n",
    "        tfidf_features           = self.vectorizers[1].transform(unseen_texts)\n",
    "        bm25_features            = self.vectorizers[2].transform(unseen_texts)\n",
    "\n",
    "        # Combine features\n",
    "        unseen_combined_features = hstack([binary_features, tfidf_features, bm25_features])\n",
    "\n",
    "        # Select features using the indices chosen during feature selection\n",
    "        unseen_selected_features = unseen_combined_features[:, self.selected_feature_indices]\n",
    "\n",
    "        # Predict sentiments\n",
    "        predictions              = model.predict(unseen_selected_features)\n",
    "\n",
    "        # Print predictions\n",
    "        print(\"Predictions on Unseen Data:\")\n",
    "        for text, pred in zip(unseen_texts, predictions):\n",
    "            print(f\"Text: {text}\\nPredicted Sentiment: {pred}\\n\")\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0217e5bc-66c8-4520-a8a4-89492faebdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "Created 20000 TF-IDF features\n"
     ]
    }
   ],
   "source": [
    "vectorizer, matrix = word_level.create_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad1ba1da-785b-4f37-9277-4e4ea2b29c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaron', 'ab', 'abandon', ..., 'zu', 'zucco', 'zucker'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66aba673-fb89-4628-9dca-f43feae19dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_selector = TextFeatureSelector(X             = matrix,\n",
    "                                   y             = imdb_ratings_df['sentiment'].values,\n",
    "                                   feature_names = (list(vectorizer.get_feature_names_out())),\n",
    "                                   n_features    = 10000,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea891f6-be77-45ce-9d5d-d3176bbf43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing chi-square feature selection...\n",
      "Selected 10000 features using chi-square\n"
     ]
    }
   ],
   "source": [
    "chi2_features, scores = new_selector.chi_square_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff6668b-5091-4ebe-9567-e1a7e96cb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = vectorizer.get_feature_names_out()[chi2_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0f01bc-2bd7-44bf-9e92-e8bac757d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fnames)):\n",
    "    fnames[i] = fnames[i].replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "398bcf61-b292-44f3-9daf-62a11120294d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['waste', 'worst', 'bad', ..., 'bond_film', 'easily_one', 'motive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca307ec6-d8d8-4636-a87c-fa152b348cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer:\n",
    "    def __init__(self, feature_names=None, weight_factor=2.0, vector_size=100, window=5, min_count=1, epochs=10):\n",
    "        \"\"\"\n",
    "        Initialize the TextVectorizer class.\n",
    "\n",
    "        :param feature_names: List of feature names (tokens) to be weighted more.\n",
    "        :param weight_factor: Weight multiplier for feature names.\n",
    "        :param vector_size: Dimensionality of word vectors.\n",
    "        :param window: Maximum distance between the current and predicted word in CBOW.\n",
    "        :param min_count: Ignores all words with total frequency lower than this.\n",
    "        :param epochs: Number of iterations (epochs) over the corpus.\n",
    "        \"\"\"\n",
    "        self.feature_names = set(feature_names) if feature_names else set()\n",
    "        self.weight_factor = weight_factor\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self.bigram_phraser = None\n",
    "        self.trigram_phraser = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the CBOW model on the given corpus.\n",
    "\n",
    "        :param corpus: List of tokenized texts (list of lists of strings).\n",
    "        \"\"\"\n",
    "        sentences = [text.split() for text in corpus]  # Tokenized text\n",
    "        bigram = Phrases(sentences, min_count=5, threshold=10)\n",
    "        trigram = Phrases(bigram[sentences], threshold=10)\n",
    "        \n",
    "        self.bigram_phraser = Phraser(bigram)\n",
    "        self.trigram_phraser = Phraser(trigram)\n",
    "        \n",
    "        # Transform sentences to include phrases\n",
    "        processed_corpus = [self.trigram_phraser[self.bigram_phraser[sentence]] for sentence in sentences]\n",
    "\n",
    "        print(\"Vectorizer training...\")\n",
    "\n",
    "        self.model = Word2Vec(\n",
    "            sentences=processed_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=0,  # CBOW model\n",
    "        )\n",
    "        \n",
    "        self.model.train(processed_corpus, total_examples=len(processed_corpus), epochs=self.epochs)\n",
    "        print(\"Vectorizer Training Complete\")\n",
    "\n",
    "    def _get_weighted_vector(self, word):\n",
    "        \"\"\"\n",
    "        Get the weighted vector for a given word.\n",
    "\n",
    "        :param word: Word for which the vector is to be retrieved.\n",
    "        :return: Weighted vector for the word.\n",
    "        \"\"\"\n",
    "        if word in self.model.wv:\n",
    "            vector = self.model.wv[word].copy()\n",
    "            if word in self.feature_names:\n",
    "                vector *= self.weight_factor\n",
    "            return vector\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def text_to_vector(self, text):\n",
    "        \"\"\"\n",
    "        Convert a text into its vector representation.\n",
    "\n",
    "        :param text: List of words (tokens) in the text.\n",
    "        :return: Vector representation of the text.\n",
    "        \"\"\"\n",
    "        vectors = [self._get_weighted_vector(word) for word in text]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)  # Average the vectors\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\"\n",
    "        Transform a list of texts into their vector representations.\n",
    "\n",
    "        :param texts: List of tokenized texts (list of lists of strings).\n",
    "        :return: List of vector representations of the texts.\n",
    "        \"\"\"\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        processed_texts = [self.trigram_phraser[self.bigram_phraser[text]] for text in tokenized_texts]\n",
    "        return [self.text_to_vector(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3a6bd7e-3612-45fc-9fe2-3a615a9f5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = TextVectorizer(feature_names=list(fnames), vector_size=100, min_count=3, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17506dde-c2b8-438b-8add-6fc485319ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer training...\n",
      "Vectorizer Training Complete\n"
     ]
    }
   ],
   "source": [
    "word_vec.train(list(imdb_ratings_df['clean_text']))\n",
    "\n",
    "# vectorized_text = word_vec.transform(list(imdb_ratings_df['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "98bbb96a-1743-48f7-b007-a92128b0bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = word_vec.transform(list(imdb_ratings_df['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "685b88b9-b1a0-4e1f-8c07-731f021660b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for training and evaluating sentiment analysis models, including testing on unseen data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the SentimentAnalyzer by splitting the data.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            X            : Feature matrix (ndarray)\n",
    "            y            : Target labels (array-like)\n",
    "            test_size    : Proportion of data to use for testing (default: 0.2)\n",
    "            random_state : Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    def train_model(self, model_type: str = \"logistic_regression\", kernel=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train a sentiment analysis model.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model_type : Type of model to train (e.g., \"logistic_regression\", \"svm\", \"random_forest\", etc.)\n",
    "            kernel     : Kernel type for SVM (e.g., \"linear\", \"poly\", \"rbf\", \"sigmoid\")\n",
    "            kwargs     : Additional arguments for model initialization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        if model_type == \"logistic_regression\":\n",
    "            model = LogisticRegression(max_iter=1000, **kwargs)\n",
    "        elif model_type == \"svm\":\n",
    "            kernel = kernel or \"rbf\"\n",
    "            model = SVC(kernel=kernel, **kwargs)\n",
    "        elif model_type == \"random_forest\":\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "        elif model_type == \"gradient_boosting\":\n",
    "            model = LGBMClassifier(**kwargs)\n",
    "        elif model_type == \"mlp\":\n",
    "            model = MLPClassifier(max_iter=500, **kwargs)\n",
    "        elif model_type == \"naive_bayes\":\n",
    "            model = MultinomialNB(**kwargs)\n",
    "        elif model_type == \"knn\":\n",
    "            model = KNeighborsClassifier(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unsupported model_type. Choose from: 'logistic_regression', 'svm', 'random_forest', \"\n",
    "                \"'gradient_boosting', 'mlp', 'naive_bayes', 'knn'.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_type}...\")\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model on the test set.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model : Trained model\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        report = classification_report(self.y_test, y_pred)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "        }\n",
    "\n",
    "    def test_on_unseen_data(self, model, unseen_texts, vectorizer_function, original_class):\n",
    "        \"\"\"\n",
    "        Test the model on unseen data.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "            model              : Trained model\n",
    "            unseen_texts       : List of unseen text data\n",
    "            vectorizer_function: Function to convert texts into vector embeddings\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            Predictions for the unseen data\n",
    "        \"\"\"\n",
    "        print(\"Processing unseen data...\")\n",
    "\n",
    "        # Convert unseen texts to vector embeddings using the provided function\n",
    "        unseen_vectors = vectorizer_function(unseen_texts)\n",
    "\n",
    "        # Predict sentiments\n",
    "        predictions = model.predict(unseen_vectors)\n",
    "\n",
    "        accuracy = accuracy_score(original_class, predictions)\n",
    "        report = classification_report(original_class, predictions)\n",
    "        cm = confusion_matrix(original_class, predictions)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b388fa10-5e8e-4323-91fa-d305a2c6bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 49/49 [00:00<00:00, 4973.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# Add a new column to the original DataFrame to store the cleaned texts\n",
    "test_df['clean_text'] = test_df['review'].progress_apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5cba22d9-d8db-442a-a7f6-c7748a9e92cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic_regression...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8601\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.85      0.86      4961\n",
      "    positive       0.85      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4217  744]\n",
      " [ 655 4384]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.7551\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.86      0.75        21\n",
      "    positive       0.86      0.68      0.76        28\n",
      "\n",
      "    accuracy                           0.76        49\n",
      "   macro avg       0.77      0.77      0.76        49\n",
      "weighted avg       0.78      0.76      0.76        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 9 19]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logistic_model = sentiment_analyzer.train_model(model_type=\"logistic_regression\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(logistic_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(logistic_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ba6dbc6-0cc4-4d5d-9a46-d3aeae0a9ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svm...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8655\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.85      0.86      4961\n",
      "    positive       0.86      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4235  726]\n",
      " [ 619 4420]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.7551\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.90      0.76        21\n",
      "    positive       0.90      0.64      0.75        28\n",
      "\n",
      "    accuracy                           0.76        49\n",
      "   macro avg       0.78      0.77      0.76        49\n",
      "weighted avg       0.80      0.76      0.75        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [10 18]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a svm rbf model\n",
    "svmrbf_model = sentiment_analyzer.train_model(model_type=\"svm\", kernel='rbf')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(svmrbf_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(svmrbf_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "611d82bb-540e-4881-9f9d-c84901e44057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svm...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8653\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.85      0.86      4961\n",
      "    positive       0.86      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4227  734]\n",
      " [ 613 4426]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.6735\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.62      0.62        21\n",
      "    positive       0.71      0.71      0.71        28\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.67      0.67      0.67        49\n",
      "weighted avg       0.67      0.67      0.67        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  8]\n",
      " [ 8 20]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a svm poly model\n",
    "svmpoly_model = sentiment_analyzer.train_model(model_type=\"svm\", kernel='poly')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(svmpoly_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(svmpoly_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d8ff45a-c7f3-4038-8d5e-66ae77f376b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svm...\n",
      "Evaluating model...\n",
      "Accuracy: 0.7210\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.72      0.72      4961\n",
      "    positive       0.72      0.72      0.72      5039\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.72      0.72      0.72     10000\n",
      "weighted avg       0.72      0.72      0.72     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3574 1387]\n",
      " [1403 3636]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.5510\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.95      0.65        21\n",
      "    positive       0.88      0.25      0.39        28\n",
      "\n",
      "    accuracy                           0.55        49\n",
      "   macro avg       0.68      0.60      0.52        49\n",
      "weighted avg       0.71      0.55      0.50        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  1]\n",
      " [21  7]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a svm sig model\n",
    "svmsig_model = sentiment_analyzer.train_model(model_type=\"svm\", kernel='sigmoid')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(svmsig_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(svmsig_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9bc29af7-b720-4d6a-a276-a8ea57a30d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random_forest...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8364\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.82      0.83      4961\n",
      "    positive       0.83      0.85      0.84      5039\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4081  880]\n",
      " [ 756 4283]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.7551\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.76      0.73        21\n",
      "    positive       0.81      0.75      0.78        28\n",
      "\n",
      "    accuracy                           0.76        49\n",
      "   macro avg       0.75      0.76      0.75        49\n",
      "weighted avg       0.76      0.76      0.76        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  5]\n",
      " [ 7 21]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a Random Forest model\n",
    "randf_model = sentiment_analyzer.train_model(model_type=\"random_forest\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(randf_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(randf_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3dda6a93-b902-46aa-ad22-c13bb4bff9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8406\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.82      0.84      4961\n",
      "    positive       0.83      0.86      0.84      5039\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4079  882]\n",
      " [ 712 4327]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.6735\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.57      0.60        21\n",
      "    positive       0.70      0.75      0.72        28\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.67      0.66      0.66        49\n",
      "weighted avg       0.67      0.67      0.67        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12  9]\n",
      " [ 7 21]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a Multi Layer Perceptron model\n",
    "mlp_model = sentiment_analyzer.train_model(model_type=\"mlp\", hidden_layer_sizes = (1000,))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(mlp_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(mlp_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8df318f4-83ba-4f4e-92f3-9a2d3a420d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training knn...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8104\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81      4961\n",
      "    positive       0.81      0.81      0.81      5039\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4036  925]\n",
      " [ 971 4068]]\n",
      "Processing unseen data...\n",
      "Accuracy: 0.6939\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.81      0.69        21\n",
      "    positive       0.81      0.61      0.69        28\n",
      "\n",
      "    accuracy                           0.69        49\n",
      "   macro avg       0.71      0.71      0.69        49\n",
      "weighted avg       0.72      0.69      0.69        49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17  4]\n",
      " [11 17]]\n"
     ]
    }
   ],
   "source": [
    "X = vectorized_text\n",
    "y = imdb_ratings_df['sentiment'].values\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer(X, y)\n",
    "\n",
    "# Train a knn model\n",
    "knn_model = sentiment_analyzer.train_model(model_type=\"knn\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = sentiment_analyzer.evaluate_model(knn_model)\n",
    "\n",
    "# Test on unseen data\n",
    "prediction_res = sentiment_analyzer.test_on_unseen_data(knn_model, list(test_df['clean_text']), word_vec.transform, list(test_df['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd9002-f73f-42b9-a2ea-1fad461967ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e046703-2b4f-4bc6-b882-73c17de126f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3922436-b39d-4869-ba85-5d83d373cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a671b16-0add-4ec1-af52-413b50baf075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
